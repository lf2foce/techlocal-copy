{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123: A New Approach',\n",
       " \"This theme explores how '123' relates to 123. We’ll break down why this is crucial and how to apply it practically.\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def generate_theme_title_and_story(campaign_title: str, insight: str) -> tuple[str, str]:\n",
    "    title_templates = [\n",
    "        \"The Power of {keyword}\",\n",
    "        \"Mastering {keyword} for Growth\",\n",
    "        \"Why {keyword} Matters Now\",\n",
    "        \"{keyword}: A New Approach\",\n",
    "        \"Unlocking {keyword}\"\n",
    "    ]\n",
    "\n",
    "    insight_keywords = insight.split()\n",
    "    keyword = random.choice(insight_keywords if insight_keywords else [\"change\"])\n",
    "    title = random.choice(title_templates).format(keyword=keyword.capitalize())\n",
    "\n",
    "    story = f\"This theme explores how '{keyword}' relates to {campaign_title.lower()}. We’ll break down why this is crucial and how to apply it practically.\"\n",
    "    return title, story\n",
    "generate_theme_title_and_story(\"123\", \"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def generate_topic():\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a creative assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Generate a unique topic for a technology blog.\"}\n",
    "        ]\n",
    "    )\n",
    "    topic = response.choices[0].message['content']\n",
    "    print(f\"Generated Topic: {topic}\")\n",
    "\n",
    "# Run the asynchronous function\n",
    "asyncio.run(generate_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 posts asynchronously with DeepSeek...\n",
      "Generated post for: Benefits of meditation\n",
      "Generated post for: How to start programming\n",
      "Generated post for: Climate change solutions\n",
      "Generated post for: Healthy meal prep ideas\n",
      "Generated post for: Financial planning tips\n",
      "Generated post for: Productivity hacks\n",
      "Generated post for: Travel on a budget\n",
      "Generated post for: Home workout routines\n",
      "Generated post for: Book recommendations for beginners\n",
      "Generated post for: Tech trends 2025\n",
      "\n",
      "Generated posts summary:\n",
      "1. Benefits of meditation - 1359 chars\n",
      "2. How to start programming - 1472 chars\n",
      "3. Climate change solutions - 1494 chars\n",
      "4. Healthy meal prep ideas - 1511 chars\n",
      "5. Financial planning tips - 1394 chars\n",
      "6. Productivity hacks - 1185 chars\n",
      "7. Travel on a budget - 1454 chars\n",
      "8. Home workout routines - 1395 chars\n",
      "9. Book recommendations for beginners - 1418 chars\n",
      "10. Tech trends 2025 - 1705 chars\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ab313/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ab313/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Export to Excel\u001b[39;00m\n\u001b[32m     83\u001b[39m excel_filename = \u001b[33m\"\u001b[39m\u001b[33mdeepseek_generated_posts.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m end_time = time.time()\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m posts to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexcel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/core/generic.py:2417\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2406\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2407\u001b[39m     df,\n\u001b[32m   2408\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2415\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2416\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load environment variables and apply nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenAI client with DeepSeek endpoint\n",
    "client = OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Define the topics for the posts\n",
    "topics = [\n",
    "    \"Benefits of meditation\",\n",
    "    \"How to start programming\",\n",
    "    \"Climate change solutions\",\n",
    "    \"Healthy meal prep ideas\",\n",
    "    \"Financial planning tips\",\n",
    "    \"Productivity hacks\",\n",
    "    \"Travel on a budget\",\n",
    "    \"Home workout routines\",\n",
    "    \"Book recommendations for beginners\",\n",
    "    \"Tech trends 2025\"\n",
    "]\n",
    "\n",
    "async def generate_post(topic):\n",
    "    \"\"\"Generate a post about a specific topic using DeepSeek.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes blog posts\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Write a short blog post about {topic}. Keep it under 300 words.\"},\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"Generated post for: {topic}\")\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating post for {topic}: {e}\")\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"content\": f\"Error: {str(e)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "async def process_topics():\n",
    "    \"\"\"Process all topics concurrently.\"\"\"\n",
    "    tasks = []\n",
    "    for topic in topics:\n",
    "        task = asyncio.create_task(generate_post(topic))\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "async def main():\n",
    "    print(\"Generating 10 posts asynchronously with DeepSeek...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = await process_topics()\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display summary of the DataFrame\n",
    "    print(\"\\nGenerated posts summary:\")\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"{index+1}. {row['topic']} - {len(row['content'])} chars\")\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_filename = \"deepseek_generated_posts.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExported {len(results)} posts to {excel_filename}\")\n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating posts asynchronously with Google Gemini...\n",
      "Generated post for: Benefits of meditation\n",
      "Generated post for: How to start programming\n",
      "Generated post for: Climate change solutions\n",
      "Generated post for: Healthy meal prep ideas\n",
      "Generated post for: Financial planning tips\n",
      "Generated post for: Productivity hacks\n",
      "Generated post for: Travel on a budget\n",
      "Generated post for: Home workout routines\n",
      "Generated post for: Book recommendations for beginners\n",
      "Generated post for: Tech trends 2025\n",
      "\n",
      "Generated posts summary:\n",
      "1. Unlock Inner Peace: The Benefits of Meditation - 1348 chars\n",
      "2. Embark on Your Coding Journey: A Beginner's Guide - 1076 chars\n",
      "3. Fighting Back: Practical Climate Change Solutions - 1221 chars\n",
      "4. Fuel Your Week: Easy & Healthy Meal Prep Ideas - 1209 chars\n",
      "5. Smart Financial Planning Tips for a Secure Future - 791 chars\n",
      "6. Boost Your Day: Simple Productivity Hacks - 900 chars\n",
      "7. Travel on a Budget: See the World Without Breaking the Bank - 1239 chars\n",
      "8. Effective Home Workout Routines - 1158 chars\n",
      "9. Page-Turners for Budding Bookworms: Great Reads to Get You Hooked - 1119 chars\n",
      "10. Tech Trends to Watch in 2025 - 1070 chars\n",
      "\n",
      "Exported 10 posts to gemini_generated_posts.xlsx\n",
      "Total time: 25.88 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# Load environment variables and apply nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define topics for generation\n",
    "topics = [\n",
    "    \"Benefits of meditation\",\n",
    "    \"How to start programming\",\n",
    "    \"Climate change solutions\",\n",
    "    \"Healthy meal prep ideas\",\n",
    "    \"Financial planning tips\",\n",
    "    \"Productivity hacks\",\n",
    "    \"Travel on a budget\",\n",
    "    \"Home workout routines\",\n",
    "    \"Book recommendations for beginners\",\n",
    "    \"Tech trends 2025\"\n",
    "]\n",
    "\n",
    "\n",
    "# System prompt - adapt this to your needs\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that writes engaging blog posts. \n",
    "Create content that is informative, well-structured, and under 300 words.\n",
    "Format your response as a JSON object with 'title' and 'content' fields.\n",
    "\"\"\"\n",
    "\n",
    "class BlogPost(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "async def generate_post_with_gemini(topic: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a post about a specific topic using Google Gemini.\"\"\"\n",
    "    try:\n",
    "        # Initialize the client similar to your implementation\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\") )\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"Write a short blog post about {topic}. Keep it under 300 words.\"\n",
    "        \n",
    "        # Generate response using Gemini API - following your implementation pattern\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'response_mime_type': 'application/json',\n",
    "                'response_schema': BlogPost,\n",
    "                'system_instruction': types.Part.from_text(text=system_prompt),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # Extract the response - similar to your approach\n",
    "        print(f\"Generated post for: {topic}\")\n",
    "        content = json.loads(response.text)\n",
    "        blog_post = BlogPost(**content)\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"title\": blog_post.title,\n",
    "            \"content\": blog_post.content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Gemini API Error: {str(e)}\")\n",
    "\n",
    "\n",
    "# This function handles creating the semaphore and processing tasks\n",
    "async def process_with_semaphore(topics: List[str], concurrency: int = 10):\n",
    "    \"\"\"Process topics with a semaphore to limit concurrent API calls.\"\"\"\n",
    "    # Use a semaphore to control concurrency\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "    \n",
    "    async def bounded_generate(topic):\n",
    "        async with semaphore:\n",
    "            return await generate_post_with_gemini(topic)\n",
    "    \n",
    "    tasks = [bounded_generate(topic) for topic in topics]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    print(\"Generating posts asynchronously with Google Gemini...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process all topics with controlled concurrency\n",
    "    results = await process_with_semaphore(topics)\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display summary of the DataFrame\n",
    "    print(\"\\nGenerated posts summary:\")\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"{index+1}. {row['title']} - {len(row['content'])} chars\")\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_filename = \"gemini_generated_posts.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExported {len(results)} posts to {excel_filename}\")\n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated post based on user prompt.\n",
      "Title: Unlocking Inner Peace: The Benefits of Meditation\n",
      "Content: In today's fast-paced world, finding moments of calm can feel like a luxury. However, meditation offers a simple yet powerful way to cultivate inner peace and improve overall well-being. Regular meditation practice provides numerous benefits for both your mind and body.\n",
      "\n",
      "One of the most significant advantages is stress reduction. Meditation helps calm the nervous system, lowering cortisol levels and promoting relaxation. This can lead to reduced anxiety and a greater sense of emotional balance. Beyond stress relief, meditation enhances focus and concentration. By training your mind to stay present, you can improve your ability to concentrate on tasks and make better decisions.\n",
      "\n",
      "Furthermore, meditation can positively impact your physical health. Studies have shown that it can lower blood pressure, improve sleep quality, and even boost your immune system. It also promotes self-awareness, allowing you to better understand your thoughts, feelings, and behaviors. This increased self-knowledge can lead to healthier relationships and a greater sense of purpose.\n",
      "\n",
      "Whether you're a beginner or an experienced practitioner, incorporating even a few minutes of daily meditation into your routine can make a profound difference in your life.\n",
      "Timestamp: 2025-04-04 01:15:44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# System prompt - adapt this to your needs\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that writes engaging blog posts. \n",
    "Create content that is informative, well-structured, and under 300 words.\n",
    "Format your response as a JSON object with 'title' and 'content' fields.\n",
    "\"\"\"\n",
    "\n",
    "class BlogPost(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "async def generate_post_with_gemini(user_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a post based on a user-provided prompt using Google Gemini.\"\"\"\n",
    "    try:\n",
    "        # Initialize the client\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        \n",
    "        # Generate response using Gemini API\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=user_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_mime_type='application/json',\n",
    "                response_schema=BlogPost,\n",
    "                system_instruction=types.Part.from_text(text=system_prompt),\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        print(\"Generated post based on user prompt.\")\n",
    "        content = json.loads(response.text)\n",
    "        \n",
    "        # Validate and parse the response using Pydantic\n",
    "        blog_post = BlogPost(**content)\n",
    "        \n",
    "        return {\n",
    "            \"title\": blog_post.title,\n",
    "            \"content\": blog_post.content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except ValidationError as ve:\n",
    "        print(f\"Validation error: {ve}\")\n",
    "        return {\n",
    "            \"title\": \"Validation Error\",\n",
    "            \"content\": f\"Validation Error: {str(ve)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating post: {e}\")\n",
    "        return {\n",
    "            \"title\": \"Error\",\n",
    "            \"content\": f\"Error: {str(e)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "user_prompt = \"Write a short blog post about the benefits of meditation. Keep it under 300 words.\"\n",
    "\n",
    "# Run the asynchronous function\n",
    "result = asyncio.run(generate_post_with_gemini(user_prompt))\n",
    "\n",
    "# Display the result\n",
    "print(\"Title:\", result[\"title\"])\n",
    "print(\"Content:\", result[\"content\"])\n",
    "print(\"Timestamp:\", result[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_image_prompts_for_post(post_content_vn: str, campaign_knowledge: str, brand_name: str, day_num: int):\n",
    "    print(Fore.CYAN + f\"\\nBước 10: Tạo các prompt ảnh theo mạch truyện cho bài đăng Ngày {day_num}...\")\n",
    "    MAX_POST_LENGTH_FOR_PROMPT = 2500\n",
    "    truncated_content = post_content_vn[:MAX_POST_LENGTH_FOR_PROMPT]\n",
    "    if len(post_content_vn) > MAX_POST_LENGTH_FOR_PROMPT:\n",
    "         print(Fore.YELLOW + f\"    (Nội dung bài viết rút gọn còn {MAX_POST_LENGTH_FOR_PROMPT} ký tự để tạo prompt ảnh)\")\n",
    "    prompt = (\n",
    "        \"Với vai trò là chuyên gia visual storytelling, hãy phân tích bài đăng tiếng Việt dưới đây.\\n\"\n",
    "        \"Dựa trên tên thương hiệu, mô tả kênh và nội dung bài đăng, tạo ra **3 đến 4 cặp prompt ảnh** minh họa các phần chính/cảm xúc quan trọng.\\n\\n\"\n",
    "        \"Mỗi cặp prompt gồm:\\n\"\n",
    "        \"1. **`part` (string)**: Nhãn tiếng Việt ngắn gọn xác định phần minh họa (VD: 'Mở đầu', 'Nỗi trăn trở', 'Điểm sáng', 'Thông điệp chính', 'Lời kết nối', 'Hành động').\\n\"\n",
    "        \"2. **`english_prompt` (string)**: Prompt **tiếng Anh** chi tiết (chủ thể, hành động, bối cảnh, ánh sáng, màu sắc, mood, phong cách [cinematic, realistic, illustration, symbolic...], tỷ lệ [--ar 16:9, --ar 1:1]).\\n\"\n",
    "        \"3. **`vietnamese_explanation` (string)**: Giải thích **tiếng Việt** ngắn gọn lý do prompt phù hợp.\\n\\n\"\n",
    "        f\"--- TÊN THƯƠNG HIỆU ---\\n{brand_name}\\n\\n\"\n",
    "        f\"--- MÔ TẢ KÊNH ---\\n{campaign_knowledge}\\n--- END KNOWLEDGE ---\\n\\n\"\n",
    "        f\"--- BÀI ĐĂNG TIẾNG VIỆT (Ngày {day_num}) ---\\n'''{truncated_content}'''\\n--- END POST ---\\n\\n\"\n",
    "        \"Yêu cầu Output:\\n\"\n",
    "        \"- Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\\n\"\n",
    "        \"- Value của 'story_prompts' là list chứa 3 hoặc 4 objects (ưu tiên 4 nếu đủ ý).\\n\"\n",
    "        \"- Mỗi object có 3 string keys: 'part', 'english_prompt', 'vietnamese_explanation', đều không rỗng.\"\n",
    "    )\n",
    "    response_data = gemini_text_generation(prompt, response_schema=True)\n",
    "\n",
    "    if not isinstance(response_data, dict) or 'story_prompts' not in response_data:\n",
    "        print(Fore.RED + f\"    Lỗi: Định dạng JSON prompt truyện không đúng (thiếu 'story_prompts'). Data: {response_data}\")\n",
    "        return None\n",
    "    prompts_list = response_data['story_prompts']\n",
    "    MIN_PROMPTS = 2\n",
    "    if not isinstance(prompts_list, list) or len(prompts_list) < MIN_PROMPTS:\n",
    "        print(Fore.RED + f\"    Lỗi: 'story_prompts' không phải list hoặc < {MIN_PROMPTS} phần tử (có {len(prompts_list) if isinstance(prompts_list, list) else 'N/A'}). Data: {response_data}\")\n",
    "        return None\n",
    "\n",
    "    valid_prompts = []\n",
    "    for i, item in enumerate(prompts_list):\n",
    "        if not (isinstance(item, dict) and\n",
    "                all(k in item for k in ['part', 'english_prompt', 'vietnamese_explanation']) and\n",
    "                isinstance(item.get('part'), str) and item.get('part').strip() and\n",
    "                isinstance(item.get('english_prompt'), str) and item.get('english_prompt').strip() and\n",
    "                isinstance(item.get('vietnamese_explanation'), str) and item.get('vietnamese_explanation').strip()):\n",
    "            print(Fore.YELLOW + f\"    Cảnh báo: Item prompt truyện thứ {i+1} không hợp lệ, bỏ qua: {item}\")\n",
    "            continue\n",
    "        else:\n",
    "            valid_prompts.append({\n",
    "                'part': item['part'].strip(),\n",
    "                'english_prompt': item['english_prompt'].strip(),\n",
    "                'vietnamese_explanation': item['vietnamese_explanation'].strip()\n",
    "            })\n",
    "\n",
    "    if len(valid_prompts) < MIN_PROMPTS:\n",
    "         print(Fore.RED + f\"    Lỗi: Sau khi lọc, chỉ còn {len(valid_prompts)} prompt truyện hợp lệ (< {MIN_PROMPTS}). Data: {response_data}\")\n",
    "         return None\n",
    "\n",
    "    print(Fore.GREEN + f\"    Đã tạo và xác thực {len(valid_prompts)} prompt ảnh theo mạch truyện cho Ngày {day_num} thành công.\")\n",
    "    return valid_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_image_prompts_for_post(post_content_vn: str, campaign_knowledge: str, brand_name: str, day_num: int):\n",
    "    print(f\"\\nBước 10: Tạo prompt ảnh cho bài đăng Ngày {day_num}...\")\n",
    "\n",
    "    truncated = post_content_vn[:2500]\n",
    "    if len(post_content_vn) > 2500:\n",
    "        print(\"    (Rút gọn bài viết xuống 2500 ký tự để tạo prompt)\")\n",
    "\n",
    "    prompt = (\n",
    "        \"Với vai trò là chuyên gia visual storytelling, hãy phân tích bài đăng tiếng Việt dưới đây.\\n\"\n",
    "        \"Dựa trên tên thương hiệu, mô tả kênh và nội dung bài đăng, tạo ra **3 đến 4 cặp prompt ảnh** minh họa các phần chính/cảm xúc quan trọng.\\n\\n\"\n",
    "        \"Mỗi cặp prompt gồm:\\n\"\n",
    "        \"1. **`part` (string)**: Nhãn tiếng Việt ngắn gọn xác định phần minh họa (VD: 'Mở đầu', 'Nỗi trăn trở', 'Điểm sáng', 'Thông điệp chính', 'Lời kết nối', 'Hành động').\\n\"\n",
    "        \"2. **`english_prompt` (string)**: Prompt **tiếng Anh** chi tiết (chủ thể, hành động, bối cảnh, ánh sáng, màu sắc, mood, phong cách [cinematic, realistic, illustration, symbolic...], tỷ lệ [--ar 16:9, --ar 1:1]).\\n\"\n",
    "        \"3. **`vietnamese_explanation` (string)**: Giải thích **tiếng Việt** ngắn gọn lý do prompt phù hợp.\\n\\n\"\n",
    "        f\"--- TÊN THƯƠNG HIỆU ---\\n{brand_name}\\n\\n\"\n",
    "        f\"--- MÔ TẢ KÊNH ---\\n{campaign_knowledge}\\n--- END KNOWLEDGE ---\\n\\n\"\n",
    "        f\"--- BÀI ĐĂNG TIẾNG VIỆT (Ngày {day_num}) ---\\n'''{truncated_content}'''\\n--- END POST ---\\n\\n\"\n",
    "        \"Yêu cầu Output:\\n\"\n",
    "        \"- Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\\n\"\n",
    "        \"- Value của 'story_prompts' là list chứa 3 hoặc 4 objects (ưu tiên 4 nếu đủ ý).\\n\"\n",
    "        \"- Mỗi object có 3 string keys: 'part', 'english_prompt', 'vietnamese_e  xplanation', đều không rỗng.\"\n",
    "    )\n",
    "\n",
    "    response = gemini_text_generation(prompt, response_schema=True)\n",
    "\n",
    "    if not isinstance(response, dict) or 'story_prompts' not in response:\n",
    "        print(\"    ❌ Lỗi: Thiếu 'story_prompts' trong JSON.\")\n",
    "        return None\n",
    "\n",
    "    prompts = response['story_prompts']\n",
    "    if not isinstance(prompts, list) or len(prompts) < 2:\n",
    "        print(f\"    ❌ Lỗi: Chỉ có {len(prompts) if isinstance(prompts, list) else 'N/A'} prompt.\")\n",
    "        return None\n",
    "\n",
    "    valid = []\n",
    "    for i, p in enumerate(prompts):\n",
    "        if all(isinstance(p.get(k), str) and p[k].strip() for k in ['part', 'english_prompt', 'vietnamese_explanation']):\n",
    "            valid.append({k: p[k].strip() for k in ['part', 'english_prompt', 'vietnamese_explanation']})\n",
    "        else:\n",
    "            print(f\"    ⚠️ Prompt {i+1} không hợp lệ, bỏ qua.\")\n",
    "\n",
    "    if len(valid) < 2:\n",
    "        print(f\"    ❌ Lỗi: Chỉ còn {len(valid)} prompt hợp lệ sau khi lọc.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"    ✅ Tạo {len(valid)} prompt ảnh thành công.\")\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "import os, json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ImagePrompt(BaseModel):\n",
    "    part: str\n",
    "    english_prompt: str\n",
    "    vietnamese_explanation: str\n",
    "\n",
    "class ImagePromptGenerate(BaseModel):\n",
    "    story_prompts: List[ImagePrompt]\n",
    "\n",
    "def gemini_text_image_prompt_generation(text: str):\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    \n",
    "    system_prompt= \"\"\" \n",
    "    Với vai trò là chuyên gia visual storytelling, hãy phân tích bài đăng tiếng Việt dưới đây.\n",
    "    Dựa trên tên thương hiệu, mô tả kênh và nội dung bài đăng, tạo ra **4 cặp prompt ảnh** minh họa các phần chính/cảm xúc quan trọng.\n",
    "    Mỗi cặp prompt gồm:\n",
    "    1. **`part` (string)**: Nhãn tiếng Việt ngắn gọn xác định phần minh họa (VD: 'Mở đầu', 'Nỗi trăn trở', 'Điểm sáng', 'Thông điệp chính', 'Lời kết nối', 'Hành động').\n",
    "    2. **`english_prompt` (string)**: Prompt **tiếng Anh** chi tiết (chủ thể, hành động, bối cảnh, ánh sáng, màu sắc, mood, phong cách [cinematic, realistic, illustration, symbolic...], tỷ lệ [--ar 16:9, --ar 1:1]).\n",
    "    3. **`vietnamese_explanation` (string)**: Giải thích **tiếng Việt** ngắn gọn lý do prompt phù hợp.\n",
    "\n",
    "    Yêu cầu Output:\n",
    "    - Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\n",
    "    - Value của 'story_prompts' là list chứa 4 objects (ưu tiên 4 nếu đủ ý).\n",
    "    - Mỗi object có 3 string keys: 'part', 'english_prompt', 'vietnamese_explanation', đều không rỗng.\n",
    "    \"\"\"\n",
    "    # Generate response using Gemini API (synchronous version)\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=f\"Trả thông tin cho nội dung sau đây: {text}\", # Tạo 5 thương hiệu cho pages với các thông tin {insight} {target_customer}. Mỗi thương hiệu phải có title và story khác nhau.\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type='application/json',\n",
    "            response_schema=ImagePromptGenerate,\n",
    "            system_instruction=types.Part.from_text(text=system_prompt),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Extract the response\n",
    "    print(\"Generated 5 themes based on user prompt.\")\n",
    "    content = json.loads(response.text)\n",
    "    \n",
    "    # Validate and parse the response using Pydantic\n",
    "    prompts = ImagePromptGenerate(**content)\n",
    "    \n",
    "    return [(prompt.part, prompt.english_prompt, prompt.vietnamese_explanation) for prompt  in prompts.story_prompts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 themes based on user prompt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mở đầu',\n",
       "  'A cinematic shot of a lone man silhouetted against a stormy sky, symbolizing inner turmoil and the challenges of maintaining self-belief in a turbulent world. Moody lighting, desaturated colors, --ar 16:9',\n",
       "  'Hình ảnh người đàn ông đơn độc đối diện với bầu trời giông bão thể hiện sự lạc lõng và thử thách trong việc giữ vững niềm tin vào bản thân.'),\n",
       " ('Giá trị cốt lõi',\n",
       "  'An illustration of a man standing at a crossroads, contemplating different paths, with symbolic representations of values like integrity, kindness, and resilience glowing softly in the background. Warm, inviting colors, illustration style, --ar 1:1',\n",
       "  'Hình ảnh người đàn ông đứng trước ngã rẽ cuộc đời, suy ngẫm về các giá trị cốt lõi như sự chính trực, lòng tốt và khả năng phục hồi.'),\n",
       " ('Hành động nhỏ',\n",
       "  'A realistic image of a man diligently working on a small project, with sunlight streaming through the window, highlighting the sense of accomplishment and progress. Realistic style, warm lighting, --ar 16:9',\n",
       "  'Hình ảnh người đàn ông tỉ mỉ thực hiện một dự án nhỏ, ánh nắng chiếu vào thể hiện cảm giác thành tựu và tiến bộ.'),\n",
       " ('Lời kết nối',\n",
       "  'A group of men gathered in a supportive community setting, sharing stories and offering encouragement, with a sense of camaraderie and mutual respect. Warm colors, soft lighting, realistic style, --ar 16:9',\n",
       "  'Hình ảnh một nhóm đàn ông tụ họp trong một cộng đồng hỗ trợ, chia sẻ câu chuyện và động viên lẫn nhau, thể hiện sự gắn kết và tôn trọng lẫn nhau.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_text_image_prompt_generation(\"\"\" \n",
    "📝 Bài Đăng Ngày 12 - Thế Hệ Di Sản\n",
    "\n",
    "Ngày 12: Xây Dựng Bản Lĩnh - Trở Thành Phiên Bản Tốt Nhất Của Chính Mình\n",
    "Quý ông trong cộng đồng Thế Hệ Di Sản,\n",
    "Hôm nay, chúng ta cùng bàn về một chủ đề cốt lõi: bản lĩnh và lòng tự trọng. Trong thời đại biến động này, giữ vững niềm tin vào bản thân, đặc biệt với đàn ông, là một thử thách không hề nhỏ.  Nhiều người trong chúng ta, dù ở độ tuổi nào, đều từng cảm thấy lạc lõng, thiếu định hướng, thậm chí nghi ngờ chính mình. Áp lực công việc, gia đình, và xã hội… dễ dàng bào mòn sự tự tin.\n",
    "Nhưng hãy nhớ: Bản lĩnh là nền tảng của thành công. Hãy tin tưởng vào chính mình!  Đây không chỉ là lời động viên, mà là chân lý cần được sống trọn vẹn.\n",
    "Vậy làm thế nào để xây dựng và củng cố bản lĩnh? Đó không phải là đích đến mà là một hành trình đòi hỏi sự nỗ lực không ngừng, sự kiên trì, và nhất là sự tự nhận thức.\n",
    "Đầu tiên, hãy xác định giá trị cốt lõi của bản thân.  Bạn là ai? Bạn muốn cống hiến gì cho cuộc đời? Hiểu rõ bản thân là bước đệm vững chắc cho lòng tự trọng.  \n",
    "Thứ hai, hãy đặt mục tiêu nhỏ, khả thi và chinh phục từng bước một. Mỗi thành công nhỏ sẽ là viên gạch xây nên tòa lâu đài bản lĩnh.  Hãy tưởng tượng về một người đàn ông bạn ngưỡng mộ. Anh ấy đã làm gì để đạt được thành tựu như ngày hôm nay? Hãy học hỏi từ những tấm gương đó.\n",
    "Ví dụ: Nếu bạn muốn cải thiện sức khỏe, hãy bắt đầu bằng việc tập thể dục 30 phút mỗi ngày.  Nếu bạn muốn thăng tiến trong sự nghiệp, hãy đặt mục tiêu hoàn thành một dự án quan trọng.  Những thành công nhỏ này sẽ tích lũy lại, tạo nên sự tự tin mạnh mẽ.\n",
    "Thứ ba, hãy học cách yêu thương và chấp nhận bản thân, cả ưu điểm và khuyết điểm.  Sai lầm là điều tất yếu. Hãy xem đó là bài học quý giá để trưởng thành.  Một người đàn ông đích thực không sợ thất bại mà biết đứng dậy sau mỗi vấp ngã.\n",
    "Cuối cùng, hãy kết nối với cộng đồng.  Chia sẻ với những người cùng chí hướng, học hỏi từ kinh nghiệm của họ sẽ giúp bạn vững vàng hơn trên con đường hoàn thiện bản thân. Thế Hệ Di Sản chính là cộng đồng mà bạn có thể tìm thấy sự hỗ trợ và động lực đó.  \n",
    "Hành trình rèn luyện bản lĩnh là một cuộc chiến thầm lặng đòi hỏi sự dũng cảm và kiên trì.  Nhưng hãy nhớ, bạn xứng đáng với hạnh phúc và thành công. 💪\n",
    "Quý ông có bí quyết nào để giữ vững bản lĩnh trong cuộc sống? Hãy chia sẻ để cùng nhau xây dựng một cộng đồng mạnh mẽ! 👇 #TheHeDiSan #BanLĩnh #NamTính #ThànhCông\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 {\"type\": \"about:blank\", \"title\": \"Bad Request\", \"detail\": \"'9:16' is not one of ['ASPECT_10_16', 'ASPECT_16_10', 'ASPECT_9_16', 'ASPECT_16_9', 'ASPECT_3_2', 'ASPECT_2_3', 'ASPECT_4_3', 'ASPECT_3_4', 'ASPECT_1_1', 'ASPECT_1_3', 'ASPECT_3_1'] - 'image_request.aspect_ratio'\", \"status\": 400}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import asyncio\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "load_dotenv()\n",
    "url = \"https://api.ideogram.ai/generate\"\n",
    "headers = {\n",
    "    \"Api-Key\": os.getenv(\"IDEO_API_KEY\"),\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"image_request\": {\n",
    "        \"prompt\": \"A serene tropical beach scene with palm trees and a lighthouse at sunset\",\n",
    "        \"aspect_ratio\": \"ASPECT_10_16\",\n",
    "        \"model\": \"V_2\",\n",
    "        \"magic_prompt_option\": \"AUTO\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"Image URL:\", data[\"data\"][0][\"url\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your original service account file\n",
    "input_path = \"service_account.json\"  # or wherever you saved it\n",
    "\n",
    "# Load the JSON\n",
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Escape the private_key field (main issue with Render envs)\n",
    "if \"private_key\" in data:\n",
    "    data[\"private_key\"] = data[\"private_key\"].replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "# Convert full JSON to single-line safe string\n",
    "render_safe_json = json.dumps(data)\n",
    "\n",
    "# Print it out for you to copy-paste into Render\n",
    "# print(\"\\n✅ COPY THIS INTO RENDER SECRET FILE:\\n\")\n",
    "# print(render_safe_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtogether\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Together\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m Together()\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma flying cat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblack-forest-labs/FLUX.1-schnell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_safety_checker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39murl)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/resources/images.py:76\u001b[0m, in \u001b[0;36mImages.generate\u001b[0;34m(self, prompt, model, steps, seed, n, height, width, negative_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[1;32m     61\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m ImageRequest(\n\u001b[1;32m     65\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     66\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     74\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ImageResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:249\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m     options: TogetherRequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ]:\n\u001b[1;32m    242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    243\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    244\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[1;32m    245\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    246\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:635\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     content \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    642\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:707\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(resp, rcode, stream_error\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "client = Together()\n",
    "\n",
    "response = client.images.generate(\n",
    "    prompt=\"a flying cat\",\n",
    "    model=\"black-forest-labs/FLUX.1-schnell\",\n",
    "    steps=4,\n",
    "    disable_safety_checker=True,\n",
    ")\n",
    "\n",
    "print(response.data[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtogether\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Together\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Together()\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCats eating popcorn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblack-forest-labs/FLUX.1-dev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mb64_json)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/resources/images.py:76\u001b[0m, in \u001b[0;36mImages.generate\u001b[0;34m(self, prompt, model, steps, seed, n, height, width, negative_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[1;32m     61\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m ImageRequest(\n\u001b[1;32m     65\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     66\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     74\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ImageResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:249\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m     options: TogetherRequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ]:\n\u001b[1;32m    242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    243\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    244\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[1;32m    245\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    246\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:635\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     content \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    642\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:707\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(resp, rcode, stream_error\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together()\n",
    "response = client.images.generate(\n",
    "    prompt=\"Cats eating popcorn\",\n",
    "    model=\"black-forest-labs/FLUX.1-dev\",\n",
    "    steps=10,\n",
    "    n=4\n",
    ")\n",
    "print(response.data[0].b64_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco, the City by the Bay, is a popular tourist destination known for its iconic landmarks, vibrant neighborhoods, and stunning natural beauty. Here are the top things to do in San Francisco:\n",
      "\n",
      "1. **Golden Gate Bridge**: Take a walk or bike ride across the iconic suspension bridge, offering breathtaking views of the San Francisco Bay and the city skyline.\n",
      "2. **Alcatraz Island**: Visit the former maximum-security prison turned national park, featuring a guided tour and stunning views of the Bay.\n",
      "3. **Fisherman's Wharf**: Explore the bustling waterfront district, known for its seafood restaurants, street performers, and souvenir shops.\n",
      "4. **Chinatown**: Wander through the vibrant and colorful streets of San Francisco's Chinatown, one of the largest and oldest in the United States.\n",
      "5. **Union Square**: Shop and dine in this bustling square, surrounded by iconic department stores, art galleries, and theaters.\n",
      "6. **Lombard Street**: Take a stroll down the \"crookedest street in the world,\" offering stunning views of the city and the Bay.\n",
      "7. **Golden Gate Park**: Visit the sprawling urban park, featuring several attractions, including the de Young Museum, California Academy of Sciences, and the Japanese Tea Garden.\n",
      "8. **Cable Cars**: Ride the historic cable cars, a San Francisco icon, to explore the city's hills and neighborhoods.\n",
      "9. **Ferry Building Marketplace**: Sample artisanal foods and drinks at this bustling marketplace, featuring local farmers, cheesemongers, and bakeries.\n",
      "10. **Haight-Ashbury**: Explore the historic neighborhood, known for its 1960s counterculture movement, vintage shops, and eclectic cafes.\n",
      "11. **The Painted Ladies**: Admire the colorful Victorian houses, a iconic San Francisco landmark, on Alamo Square.\n",
      "12. **Museums**: Visit the San Francisco Museum of Modern Art (SFMOMA), the de Young Museum, or the Legion of Honor, featuring an impressive collection of art and artifacts.\n",
      "13. **Take a Food Tour**: Sample the city's diverse culinary scene, from seafood to sourdough bread, on a guided food tour.\n",
      "14. **Explore Neighborhoods**: Discover the unique character of San Francisco's neighborhoods, such as the Mission District, North Beach, and the Castro.\n",
      "15. **Day Trips**: Visit nearby attractions, like Muir Woods, Napa Valley, or Silicon Valley, for a day of wine tasting, hiking, or tech exploration.\n",
      "16. **Attend a Sporting Event**: Catch a game by the San Francisco Giants (baseball) or 49ers (football) at Oracle Park or Levi's Stadium.\n",
      "17. **Visit the Zoo**: Spend a day at the San Francisco Zoo, home to over 250 species of animals, including penguins, giraffes, and lions.\n",
      "18. **Take a Bay Cruise**: Enjoy a relaxing cruise on the San Francisco Bay, offering stunning views of the city skyline and the Golden Gate Bridge.\n",
      "19. **Explore the Presidio**: Visit the former military base turned national park, featuring hiking trails, scenic overlooks, and the Walt Disney Family Museum.\n",
      "20. **Attend a Festival or Event**: San Francisco hosts various festivals throughout the year, such as Outside Lands, SF Pride, and the San Francisco Jazz Festival.\n",
      "\n",
      "These are just a few of the many amazing things to do in San Francisco. With so much to see and do, you're sure to have an unforgettable experience in this vibrant and beautiful city.\n",
      "Paris is the capital and most populous city of France.\n"
     ]
    }
   ],
   "source": [
    "import os, asyncio\n",
    "from together import AsyncTogether\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "messages = [\n",
    "    \"What are the top things to do in San Francisco?\",\n",
    "    \"What country is Paris in?\",\n",
    "]\n",
    "\n",
    "async def async_chat_completion(messages):\n",
    "    async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "    tasks = [\n",
    "        async_client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        )\n",
    "        for message in messages\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    for response in responses:\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "asyncio.run(async_chat_completion(messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.together.ai/imgproxy/drNN5ZL8IcVzm1jzMqp31hg6jbpyVmo5AmdxUnm2I4Q/format:jpeg/aHR0cHM6Ly90b2dldGhlci1haS1iZmwtaW1hZ2VzLXByb2QuczMudXMtd2VzdC0yLmFtYXpvbmF3cy5jb20vaW1hZ2VzLzM0NmU0MGQ5NGI0M2JlZjg3YmUzYjdlZDNhYTgyZjFiMmZlMzUwYWE5MTUwZWEwYzg2MDIwNGZlNzhjZjVmOTk_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\n",
      "https://api.together.ai/imgproxy/AB7tLA8nyEqjB3aD9ULdHw-Y1teSNtNa-zsA3LKg65M/format:jpeg/aHR0cHM6Ly90b2dldGhlci1haS1iZmwtaW1hZ2VzLXByb2QuczMudXMtd2VzdC0yLmFtYXpvbmF3cy5jb20vaW1hZ2VzL2I4ZGM3M2U1NGFiNzYxZmY2ZTY0M2E0YWJlYzIwOWEzODYzMDRhZmYxMmQzZTI4YjlhY2IzZWE4YWM3MjI4NTA_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\n"
     ]
    }
   ],
   "source": [
    "# List of prompts for image generation\n",
    "prompts = [\n",
    "    \"A futuristic cityscape at dusk\",\n",
    "    \"A serene mountain landscape in autumn\",\n",
    "]\n",
    "\n",
    "# Asynchronous function to generate images for each prompt\n",
    "async def async_image_generation(prompts):\n",
    "    tasks = [\n",
    "        async_client.images.generate(\n",
    "            prompt=prompt,\n",
    "            model=\"black-forest-labs/FLUX.1-schnell\",\n",
    "            width=576,   # 9:16 aspect ratio\n",
    "            height=1024,\n",
    "            steps=4,\n",
    "            n=1,\n",
    "        )\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Print the URL of each generated image\n",
    "    for response in responses:\n",
    "        print(response.data[0].url)\n",
    "\n",
    "# Run the asynchronous image generation function\n",
    "asyncio.run(async_image_generation(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "response = client.images.generate(\n",
    "    prompt=\"space robots\",\n",
    "    model=\"black-forest-labs/FLUX.1-schnell\",\n",
    "    steps=10,\n",
    "    n=2,\n",
    ")\n",
    "print(response.data[0].b64_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ab312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
