{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123: A New Approach',\n",
       " \"This theme explores how '123' relates to 123. We‚Äôll break down why this is crucial and how to apply it practically.\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def generate_theme_title_and_story(campaign_title: str, insight: str) -> tuple[str, str]:\n",
    "    title_templates = [\n",
    "        \"The Power of {keyword}\",\n",
    "        \"Mastering {keyword} for Growth\",\n",
    "        \"Why {keyword} Matters Now\",\n",
    "        \"{keyword}: A New Approach\",\n",
    "        \"Unlocking {keyword}\"\n",
    "    ]\n",
    "\n",
    "    insight_keywords = insight.split()\n",
    "    keyword = random.choice(insight_keywords if insight_keywords else [\"change\"])\n",
    "    title = random.choice(title_templates).format(keyword=keyword.capitalize())\n",
    "\n",
    "    story = f\"This theme explores how '{keyword}' relates to {campaign_title.lower()}. We‚Äôll break down why this is crucial and how to apply it practically.\"\n",
    "    return title, story\n",
    "generate_theme_title_and_story(\"123\", \"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def generate_topic():\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a creative assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Generate a unique topic for a technology blog.\"}\n",
    "        ]\n",
    "    )\n",
    "    topic = response.choices[0].message['content']\n",
    "    print(f\"Generated Topic: {topic}\")\n",
    "\n",
    "# Run the asynchronous function\n",
    "asyncio.run(generate_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 posts asynchronously with DeepSeek...\n",
      "Generated post for: Benefits of meditation\n",
      "Generated post for: How to start programming\n",
      "Generated post for: Climate change solutions\n",
      "Generated post for: Healthy meal prep ideas\n",
      "Generated post for: Financial planning tips\n",
      "Generated post for: Productivity hacks\n",
      "Generated post for: Travel on a budget\n",
      "Generated post for: Home workout routines\n",
      "Generated post for: Book recommendations for beginners\n",
      "Generated post for: Tech trends 2025\n",
      "\n",
      "Generated posts summary:\n",
      "1. Benefits of meditation - 1359 chars\n",
      "2. How to start programming - 1472 chars\n",
      "3. Climate change solutions - 1494 chars\n",
      "4. Healthy meal prep ideas - 1511 chars\n",
      "5. Financial planning tips - 1394 chars\n",
      "6. Productivity hacks - 1185 chars\n",
      "7. Travel on a budget - 1454 chars\n",
      "8. Home workout routines - 1395 chars\n",
      "9. Book recommendations for beginners - 1418 chars\n",
      "10. Tech trends 2025 - 1705 chars\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ab313/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ab313/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Export to Excel\u001b[39;00m\n\u001b[32m     83\u001b[39m excel_filename = \u001b[33m\"\u001b[39m\u001b[33mdeepseek_generated_posts.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m end_time = time.time()\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m posts to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexcel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/core/generic.py:2417\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2406\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2407\u001b[39m     df,\n\u001b[32m   2408\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2415\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2416\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load environment variables and apply nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenAI client with DeepSeek endpoint\n",
    "client = OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Define the topics for the posts\n",
    "topics = [\n",
    "    \"Benefits of meditation\",\n",
    "    \"How to start programming\",\n",
    "    \"Climate change solutions\",\n",
    "    \"Healthy meal prep ideas\",\n",
    "    \"Financial planning tips\",\n",
    "    \"Productivity hacks\",\n",
    "    \"Travel on a budget\",\n",
    "    \"Home workout routines\",\n",
    "    \"Book recommendations for beginners\",\n",
    "    \"Tech trends 2025\"\n",
    "]\n",
    "\n",
    "async def generate_post(topic):\n",
    "    \"\"\"Generate a post about a specific topic using DeepSeek.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes blog posts\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Write a short blog post about {topic}. Keep it under 300 words.\"},\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"Generated post for: {topic}\")\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating post for {topic}: {e}\")\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"content\": f\"Error: {str(e)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "async def process_topics():\n",
    "    \"\"\"Process all topics concurrently.\"\"\"\n",
    "    tasks = []\n",
    "    for topic in topics:\n",
    "        task = asyncio.create_task(generate_post(topic))\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "async def main():\n",
    "    print(\"Generating 10 posts asynchronously with DeepSeek...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = await process_topics()\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display summary of the DataFrame\n",
    "    print(\"\\nGenerated posts summary:\")\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"{index+1}. {row['topic']} - {len(row['content'])} chars\")\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_filename = \"deepseek_generated_posts.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExported {len(results)} posts to {excel_filename}\")\n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating posts asynchronously with Google Gemini...\n",
      "Generated post for: Benefits of meditation\n",
      "Generated post for: How to start programming\n",
      "Generated post for: Climate change solutions\n",
      "Generated post for: Healthy meal prep ideas\n",
      "Generated post for: Financial planning tips\n",
      "Generated post for: Productivity hacks\n",
      "Generated post for: Travel on a budget\n",
      "Generated post for: Home workout routines\n",
      "Generated post for: Book recommendations for beginners\n",
      "Generated post for: Tech trends 2025\n",
      "\n",
      "Generated posts summary:\n",
      "1. Unlock Inner Peace: The Benefits of Meditation - 1348 chars\n",
      "2. Embark on Your Coding Journey: A Beginner's Guide - 1076 chars\n",
      "3. Fighting Back: Practical Climate Change Solutions - 1221 chars\n",
      "4. Fuel Your Week: Easy & Healthy Meal Prep Ideas - 1209 chars\n",
      "5. Smart Financial Planning Tips for a Secure Future - 791 chars\n",
      "6. Boost Your Day: Simple Productivity Hacks - 900 chars\n",
      "7. Travel on a Budget: See the World Without Breaking the Bank - 1239 chars\n",
      "8. Effective Home Workout Routines - 1158 chars\n",
      "9. Page-Turners for Budding Bookworms: Great Reads to Get You Hooked - 1119 chars\n",
      "10. Tech Trends to Watch in 2025 - 1070 chars\n",
      "\n",
      "Exported 10 posts to gemini_generated_posts.xlsx\n",
      "Total time: 25.88 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# Load environment variables and apply nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define topics for generation\n",
    "topics = [\n",
    "    \"Benefits of meditation\",\n",
    "    \"How to start programming\",\n",
    "    \"Climate change solutions\",\n",
    "    \"Healthy meal prep ideas\",\n",
    "    \"Financial planning tips\",\n",
    "    \"Productivity hacks\",\n",
    "    \"Travel on a budget\",\n",
    "    \"Home workout routines\",\n",
    "    \"Book recommendations for beginners\",\n",
    "    \"Tech trends 2025\"\n",
    "]\n",
    "\n",
    "\n",
    "# System prompt - adapt this to your needs\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that writes engaging blog posts. \n",
    "Create content that is informative, well-structured, and under 300 words.\n",
    "Format your response as a JSON object with 'title' and 'content' fields.\n",
    "\"\"\"\n",
    "\n",
    "class BlogPost(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "async def generate_post_with_gemini(topic: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a post about a specific topic using Google Gemini.\"\"\"\n",
    "    try:\n",
    "        # Initialize the client similar to your implementation\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\") )\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"Write a short blog post about {topic}. Keep it under 300 words.\"\n",
    "        \n",
    "        # Generate response using Gemini API - following your implementation pattern\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'response_mime_type': 'application/json',\n",
    "                'response_schema': BlogPost,\n",
    "                'system_instruction': types.Part.from_text(text=system_prompt),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # Extract the response - similar to your approach\n",
    "        print(f\"Generated post for: {topic}\")\n",
    "        content = json.loads(response.text)\n",
    "        blog_post = BlogPost(**content)\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"title\": blog_post.title,\n",
    "            \"content\": blog_post.content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Gemini API Error: {str(e)}\")\n",
    "\n",
    "\n",
    "# This function handles creating the semaphore and processing tasks\n",
    "async def process_with_semaphore(topics: List[str], concurrency: int = 10):\n",
    "    \"\"\"Process topics with a semaphore to limit concurrent API calls.\"\"\"\n",
    "    # Use a semaphore to control concurrency\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "    \n",
    "    async def bounded_generate(topic):\n",
    "        async with semaphore:\n",
    "            return await generate_post_with_gemini(topic)\n",
    "    \n",
    "    tasks = [bounded_generate(topic) for topic in topics]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    print(\"Generating posts asynchronously with Google Gemini...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process all topics with controlled concurrency\n",
    "    results = await process_with_semaphore(topics)\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display summary of the DataFrame\n",
    "    print(\"\\nGenerated posts summary:\")\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"{index+1}. {row['title']} - {len(row['content'])} chars\")\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_filename = \"gemini_generated_posts.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExported {len(results)} posts to {excel_filename}\")\n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated post based on user prompt.\n",
      "Title: Unlocking Inner Peace: The Benefits of Meditation\n",
      "Content: In today's fast-paced world, finding moments of calm can feel like a luxury. However, meditation offers a simple yet powerful way to cultivate inner peace and improve overall well-being. Regular meditation practice provides numerous benefits for both your mind and body.\n",
      "\n",
      "One of the most significant advantages is stress reduction. Meditation helps calm the nervous system, lowering cortisol levels and promoting relaxation. This can lead to reduced anxiety and a greater sense of emotional balance. Beyond stress relief, meditation enhances focus and concentration. By training your mind to stay present, you can improve your ability to concentrate on tasks and make better decisions.\n",
      "\n",
      "Furthermore, meditation can positively impact your physical health. Studies have shown that it can lower blood pressure, improve sleep quality, and even boost your immune system. It also promotes self-awareness, allowing you to better understand your thoughts, feelings, and behaviors. This increased self-knowledge can lead to healthier relationships and a greater sense of purpose.\n",
      "\n",
      "Whether you're a beginner or an experienced practitioner, incorporating even a few minutes of daily meditation into your routine can make a profound difference in your life.\n",
      "Timestamp: 2025-04-04 01:15:44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# System prompt - adapt this to your needs\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that writes engaging blog posts. \n",
    "Create content that is informative, well-structured, and under 300 words.\n",
    "Format your response as a JSON object with 'title' and 'content' fields.\n",
    "\"\"\"\n",
    "\n",
    "class BlogPost(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "async def generate_post_with_gemini(user_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a post based on a user-provided prompt using Google Gemini.\"\"\"\n",
    "    try:\n",
    "        # Initialize the client\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        \n",
    "        # Generate response using Gemini API\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=user_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_mime_type='application/json',\n",
    "                response_schema=BlogPost,\n",
    "                system_instruction=types.Part.from_text(text=system_prompt),\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        print(\"Generated post based on user prompt.\")\n",
    "        content = json.loads(response.text)\n",
    "        \n",
    "        # Validate and parse the response using Pydantic\n",
    "        blog_post = BlogPost(**content)\n",
    "        \n",
    "        return {\n",
    "            \"title\": blog_post.title,\n",
    "            \"content\": blog_post.content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except ValidationError as ve:\n",
    "        print(f\"Validation error: {ve}\")\n",
    "        return {\n",
    "            \"title\": \"Validation Error\",\n",
    "            \"content\": f\"Validation Error: {str(ve)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating post: {e}\")\n",
    "        return {\n",
    "            \"title\": \"Error\",\n",
    "            \"content\": f\"Error: {str(e)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "user_prompt = \"Write a short blog post about the benefits of meditation. Keep it under 300 words.\"\n",
    "\n",
    "# Run the asynchronous function\n",
    "result = asyncio.run(generate_post_with_gemini(user_prompt))\n",
    "\n",
    "# Display the result\n",
    "print(\"Title:\", result[\"title\"])\n",
    "print(\"Content:\", result[\"content\"])\n",
    "print(\"Timestamp:\", result[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_image_prompts_for_post(post_content_vn: str, campaign_knowledge: str, brand_name: str, day_num: int):\n",
    "    print(Fore.CYAN + f\"\\nB∆∞·ªõc 10: T·∫°o c√°c prompt ·∫£nh theo m·∫°ch truy·ªán cho b√†i ƒëƒÉng Ng√†y {day_num}...\")\n",
    "    MAX_POST_LENGTH_FOR_PROMPT = 2500\n",
    "    truncated_content = post_content_vn[:MAX_POST_LENGTH_FOR_PROMPT]\n",
    "    if len(post_content_vn) > MAX_POST_LENGTH_FOR_PROMPT:\n",
    "         print(Fore.YELLOW + f\"    (N·ªôi dung b√†i vi·∫øt r√∫t g·ªçn c√≤n {MAX_POST_LENGTH_FOR_PROMPT} k√Ω t·ª± ƒë·ªÉ t·∫°o prompt ·∫£nh)\")\n",
    "    prompt = (\n",
    "        \"V·ªõi vai tr√≤ l√† chuy√™n gia visual storytelling, h√£y ph√¢n t√≠ch b√†i ƒëƒÉng ti·∫øng Vi·ªát d∆∞·ªõi ƒë√¢y.\\n\"\n",
    "        \"D·ª±a tr√™n t√™n th∆∞∆°ng hi·ªáu, m√¥ t·∫£ k√™nh v√† n·ªôi dung b√†i ƒëƒÉng, t·∫°o ra **3 ƒë·∫øn 4 c·∫∑p prompt ·∫£nh** minh h·ªça c√°c ph·∫ßn ch√≠nh/c·∫£m x√∫c quan tr·ªçng.\\n\\n\"\n",
    "        \"M·ªói c·∫∑p prompt g·ªìm:\\n\"\n",
    "        \"1. **`part` (string)**: Nh√£n ti·∫øng Vi·ªát ng·∫Øn g·ªçn x√°c ƒë·ªãnh ph·∫ßn minh h·ªça (VD: 'M·ªü ƒë·∫ßu', 'N·ªói trƒÉn tr·ªü', 'ƒêi·ªÉm s√°ng', 'Th√¥ng ƒëi·ªáp ch√≠nh', 'L·ªùi k·∫øt n·ªëi', 'H√†nh ƒë·ªông').\\n\"\n",
    "        \"2. **`english_prompt` (string)**: Prompt **ti·∫øng Anh** chi ti·∫øt (ch·ªß th·ªÉ, h√†nh ƒë·ªông, b·ªëi c·∫£nh, √°nh s√°ng, m√†u s·∫Øc, mood, phong c√°ch [cinematic, realistic, illustration, symbolic...], t·ª∑ l·ªá [--ar 16:9, --ar 1:1]).\\n\"\n",
    "        \"3. **`vietnamese_explanation` (string)**: Gi·∫£i th√≠ch **ti·∫øng Vi·ªát** ng·∫Øn g·ªçn l√Ω do prompt ph√π h·ª£p.\\n\\n\"\n",
    "        f\"--- T√äN TH∆Ø∆†NG HI·ªÜU ---\\n{brand_name}\\n\\n\"\n",
    "        f\"--- M√î T·∫¢ K√äNH ---\\n{campaign_knowledge}\\n--- END KNOWLEDGE ---\\n\\n\"\n",
    "        f\"--- B√ÄI ƒêƒÇNG TI·∫æNG VI·ªÜT (Ng√†y {day_num}) ---\\n'''{truncated_content}'''\\n--- END POST ---\\n\\n\"\n",
    "        \"Y√™u c·∫ßu Output:\\n\"\n",
    "        \"- Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\\n\"\n",
    "        \"- Value c·ªßa 'story_prompts' l√† list ch·ª©a 3 ho·∫∑c 4 objects (∆∞u ti√™n 4 n·∫øu ƒë·ªß √Ω).\\n\"\n",
    "        \"- M·ªói object c√≥ 3 string keys: 'part', 'english_prompt', 'vietnamese_explanation', ƒë·ªÅu kh√¥ng r·ªóng.\"\n",
    "    )\n",
    "    response_data = gemini_text_generation(prompt, response_schema=True)\n",
    "\n",
    "    if not isinstance(response_data, dict) or 'story_prompts' not in response_data:\n",
    "        print(Fore.RED + f\"    L·ªói: ƒê·ªãnh d·∫°ng JSON prompt truy·ªán kh√¥ng ƒë√∫ng (thi·∫øu 'story_prompts'). Data: {response_data}\")\n",
    "        return None\n",
    "    prompts_list = response_data['story_prompts']\n",
    "    MIN_PROMPTS = 2\n",
    "    if not isinstance(prompts_list, list) or len(prompts_list) < MIN_PROMPTS:\n",
    "        print(Fore.RED + f\"    L·ªói: 'story_prompts' kh√¥ng ph·∫£i list ho·∫∑c < {MIN_PROMPTS} ph·∫ßn t·ª≠ (c√≥ {len(prompts_list) if isinstance(prompts_list, list) else 'N/A'}). Data: {response_data}\")\n",
    "        return None\n",
    "\n",
    "    valid_prompts = []\n",
    "    for i, item in enumerate(prompts_list):\n",
    "        if not (isinstance(item, dict) and\n",
    "                all(k in item for k in ['part', 'english_prompt', 'vietnamese_explanation']) and\n",
    "                isinstance(item.get('part'), str) and item.get('part').strip() and\n",
    "                isinstance(item.get('english_prompt'), str) and item.get('english_prompt').strip() and\n",
    "                isinstance(item.get('vietnamese_explanation'), str) and item.get('vietnamese_explanation').strip()):\n",
    "            print(Fore.YELLOW + f\"    C·∫£nh b√°o: Item prompt truy·ªán th·ª© {i+1} kh√¥ng h·ª£p l·ªá, b·ªè qua: {item}\")\n",
    "            continue\n",
    "        else:\n",
    "            valid_prompts.append({\n",
    "                'part': item['part'].strip(),\n",
    "                'english_prompt': item['english_prompt'].strip(),\n",
    "                'vietnamese_explanation': item['vietnamese_explanation'].strip()\n",
    "            })\n",
    "\n",
    "    if len(valid_prompts) < MIN_PROMPTS:\n",
    "         print(Fore.RED + f\"    L·ªói: Sau khi l·ªçc, ch·ªâ c√≤n {len(valid_prompts)} prompt truy·ªán h·ª£p l·ªá (< {MIN_PROMPTS}). Data: {response_data}\")\n",
    "         return None\n",
    "\n",
    "    print(Fore.GREEN + f\"    ƒê√£ t·∫°o v√† x√°c th·ª±c {len(valid_prompts)} prompt ·∫£nh theo m·∫°ch truy·ªán cho Ng√†y {day_num} th√†nh c√¥ng.\")\n",
    "    return valid_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_image_prompts_for_post(post_content_vn: str, campaign_knowledge: str, brand_name: str, day_num: int):\n",
    "    print(f\"\\nB∆∞·ªõc 10: T·∫°o prompt ·∫£nh cho b√†i ƒëƒÉng Ng√†y {day_num}...\")\n",
    "\n",
    "    truncated = post_content_vn[:2500]\n",
    "    if len(post_content_vn) > 2500:\n",
    "        print(\"    (R√∫t g·ªçn b√†i vi·∫øt xu·ªëng 2500 k√Ω t·ª± ƒë·ªÉ t·∫°o prompt)\")\n",
    "\n",
    "    prompt = (\n",
    "        \"V·ªõi vai tr√≤ l√† chuy√™n gia visual storytelling, h√£y ph√¢n t√≠ch b√†i ƒëƒÉng ti·∫øng Vi·ªát d∆∞·ªõi ƒë√¢y.\\n\"\n",
    "        \"D·ª±a tr√™n t√™n th∆∞∆°ng hi·ªáu, m√¥ t·∫£ k√™nh v√† n·ªôi dung b√†i ƒëƒÉng, t·∫°o ra **3 ƒë·∫øn 4 c·∫∑p prompt ·∫£nh** minh h·ªça c√°c ph·∫ßn ch√≠nh/c·∫£m x√∫c quan tr·ªçng.\\n\\n\"\n",
    "        \"M·ªói c·∫∑p prompt g·ªìm:\\n\"\n",
    "        \"1. **`part` (string)**: Nh√£n ti·∫øng Vi·ªát ng·∫Øn g·ªçn x√°c ƒë·ªãnh ph·∫ßn minh h·ªça (VD: 'M·ªü ƒë·∫ßu', 'N·ªói trƒÉn tr·ªü', 'ƒêi·ªÉm s√°ng', 'Th√¥ng ƒëi·ªáp ch√≠nh', 'L·ªùi k·∫øt n·ªëi', 'H√†nh ƒë·ªông').\\n\"\n",
    "        \"2. **`english_prompt` (string)**: Prompt **ti·∫øng Anh** chi ti·∫øt (ch·ªß th·ªÉ, h√†nh ƒë·ªông, b·ªëi c·∫£nh, √°nh s√°ng, m√†u s·∫Øc, mood, phong c√°ch [cinematic, realistic, illustration, symbolic...], t·ª∑ l·ªá [--ar 16:9, --ar 1:1]).\\n\"\n",
    "        \"3. **`vietnamese_explanation` (string)**: Gi·∫£i th√≠ch **ti·∫øng Vi·ªát** ng·∫Øn g·ªçn l√Ω do prompt ph√π h·ª£p.\\n\\n\"\n",
    "        f\"--- T√äN TH∆Ø∆†NG HI·ªÜU ---\\n{brand_name}\\n\\n\"\n",
    "        f\"--- M√î T·∫¢ K√äNH ---\\n{campaign_knowledge}\\n--- END KNOWLEDGE ---\\n\\n\"\n",
    "        f\"--- B√ÄI ƒêƒÇNG TI·∫æNG VI·ªÜT (Ng√†y {day_num}) ---\\n'''{truncated_content}'''\\n--- END POST ---\\n\\n\"\n",
    "        \"Y√™u c·∫ßu Output:\\n\"\n",
    "        \"- Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\\n\"\n",
    "        \"- Value c·ªßa 'story_prompts' l√† list ch·ª©a 3 ho·∫∑c 4 objects (∆∞u ti√™n 4 n·∫øu ƒë·ªß √Ω).\\n\"\n",
    "        \"- M·ªói object c√≥ 3 string keys: 'part', 'english_prompt', 'vietnamese_e  xplanation', ƒë·ªÅu kh√¥ng r·ªóng.\"\n",
    "    )\n",
    "\n",
    "    response = gemini_text_generation(prompt, response_schema=True)\n",
    "\n",
    "    if not isinstance(response, dict) or 'story_prompts' not in response:\n",
    "        print(\"    ‚ùå L·ªói: Thi·∫øu 'story_prompts' trong JSON.\")\n",
    "        return None\n",
    "\n",
    "    prompts = response['story_prompts']\n",
    "    if not isinstance(prompts, list) or len(prompts) < 2:\n",
    "        print(f\"    ‚ùå L·ªói: Ch·ªâ c√≥ {len(prompts) if isinstance(prompts, list) else 'N/A'} prompt.\")\n",
    "        return None\n",
    "\n",
    "    valid = []\n",
    "    for i, p in enumerate(prompts):\n",
    "        if all(isinstance(p.get(k), str) and p[k].strip() for k in ['part', 'english_prompt', 'vietnamese_explanation']):\n",
    "            valid.append({k: p[k].strip() for k in ['part', 'english_prompt', 'vietnamese_explanation']})\n",
    "        else:\n",
    "            print(f\"    ‚ö†Ô∏è Prompt {i+1} kh√¥ng h·ª£p l·ªá, b·ªè qua.\")\n",
    "\n",
    "    if len(valid) < 2:\n",
    "        print(f\"    ‚ùå L·ªói: Ch·ªâ c√≤n {len(valid)} prompt h·ª£p l·ªá sau khi l·ªçc.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"    ‚úÖ T·∫°o {len(valid)} prompt ·∫£nh th√†nh c√¥ng.\")\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "import os, json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ImagePrompt(BaseModel):\n",
    "    part: str\n",
    "    english_prompt: str\n",
    "    vietnamese_explanation: str\n",
    "\n",
    "class ImagePromptGenerate(BaseModel):\n",
    "    story_prompts: List[ImagePrompt]\n",
    "\n",
    "def gemini_text_image_prompt_generation(text: str):\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    \n",
    "    system_prompt= \"\"\" \n",
    "    V·ªõi vai tr√≤ l√† chuy√™n gia visual storytelling, h√£y ph√¢n t√≠ch b√†i ƒëƒÉng ti·∫øng Vi·ªát d∆∞·ªõi ƒë√¢y.\n",
    "    D·ª±a tr√™n t√™n th∆∞∆°ng hi·ªáu, m√¥ t·∫£ k√™nh v√† n·ªôi dung b√†i ƒëƒÉng, t·∫°o ra **4 c·∫∑p prompt ·∫£nh** minh h·ªça c√°c ph·∫ßn ch√≠nh/c·∫£m x√∫c quan tr·ªçng.\n",
    "    M·ªói c·∫∑p prompt g·ªìm:\n",
    "    1. **`part` (string)**: Nh√£n ti·∫øng Vi·ªát ng·∫Øn g·ªçn x√°c ƒë·ªãnh ph·∫ßn minh h·ªça (VD: 'M·ªü ƒë·∫ßu', 'N·ªói trƒÉn tr·ªü', 'ƒêi·ªÉm s√°ng', 'Th√¥ng ƒëi·ªáp ch√≠nh', 'L·ªùi k·∫øt n·ªëi', 'H√†nh ƒë·ªông').\n",
    "    2. **`english_prompt` (string)**: Prompt **ti·∫øng Anh** chi ti·∫øt (ch·ªß th·ªÉ, h√†nh ƒë·ªông, b·ªëi c·∫£nh, √°nh s√°ng, m√†u s·∫Øc, mood, phong c√°ch [cinematic, realistic, illustration, symbolic...], t·ª∑ l·ªá [--ar 16:9, --ar 1:1]).\n",
    "    3. **`vietnamese_explanation` (string)**: Gi·∫£i th√≠ch **ti·∫øng Vi·ªát** ng·∫Øn g·ªçn l√Ω do prompt ph√π h·ª£p.\n",
    "\n",
    "    Y√™u c·∫ßu Output:\n",
    "    - Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\n",
    "    - Value c·ªßa 'story_prompts' l√† list ch·ª©a 4 objects (∆∞u ti√™n 4 n·∫øu ƒë·ªß √Ω).\n",
    "    - M·ªói object c√≥ 3 string keys: 'part', 'english_prompt', 'vietnamese_explanation', ƒë·ªÅu kh√¥ng r·ªóng.\n",
    "    \"\"\"\n",
    "    # Generate response using Gemini API (synchronous version)\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=f\"Tr·∫£ th√¥ng tin cho n·ªôi dung sau ƒë√¢y: {text}\", # T·∫°o 5 th∆∞∆°ng hi·ªáu cho pages v·ªõi c√°c th√¥ng tin {insight} {target_customer}. M·ªói th∆∞∆°ng hi·ªáu ph·∫£i c√≥ title v√† story kh√°c nhau.\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type='application/json',\n",
    "            response_schema=ImagePromptGenerate,\n",
    "            system_instruction=types.Part.from_text(text=system_prompt),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Extract the response\n",
    "    print(\"Generated 5 themes based on user prompt.\")\n",
    "    content = json.loads(response.text)\n",
    "    \n",
    "    # Validate and parse the response using Pydantic\n",
    "    prompts = ImagePromptGenerate(**content)\n",
    "    \n",
    "    return [(prompt.part, prompt.english_prompt, prompt.vietnamese_explanation) for prompt  in prompts.story_prompts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 themes based on user prompt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('M·ªü ƒë·∫ßu',\n",
       "  'A cinematic shot of a lone man silhouetted against a stormy sky, symbolizing inner turmoil and the challenges of maintaining self-belief in a turbulent world. Moody lighting, desaturated colors, --ar 16:9',\n",
       "  'H√¨nh ·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒë∆°n ƒë·ªôc ƒë·ªëi di·ªán v·ªõi b·∫ßu tr·ªùi gi√¥ng b√£o th·ªÉ hi·ªán s·ª± l·∫°c l√µng v√† th·ª≠ th√°ch trong vi·ªác gi·ªØ v·ªØng ni·ªÅm tin v√†o b·∫£n th√¢n.'),\n",
       " ('Gi√° tr·ªã c·ªët l√µi',\n",
       "  'An illustration of a man standing at a crossroads, contemplating different paths, with symbolic representations of values like integrity, kindness, and resilience glowing softly in the background. Warm, inviting colors, illustration style, --ar 1:1',\n",
       "  'H√¨nh ·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒë·ª©ng tr∆∞·ªõc ng√£ r·∫Ω cu·ªôc ƒë·ªùi, suy ng·∫´m v·ªÅ c√°c gi√° tr·ªã c·ªët l√µi nh∆∞ s·ª± ch√≠nh tr·ª±c, l√≤ng t·ªët v√† kh·∫£ nƒÉng ph·ª•c h·ªìi.'),\n",
       " ('H√†nh ƒë·ªông nh·ªè',\n",
       "  'A realistic image of a man diligently working on a small project, with sunlight streaming through the window, highlighting the sense of accomplishment and progress. Realistic style, warm lighting, --ar 16:9',\n",
       "  'H√¨nh ·∫£nh ng∆∞·ªùi ƒë√†n √¥ng t·ªâ m·ªâ th·ª±c hi·ªán m·ªôt d·ª± √°n nh·ªè, √°nh n·∫Øng chi·∫øu v√†o th·ªÉ hi·ªán c·∫£m gi√°c th√†nh t·ª±u v√† ti·∫øn b·ªô.'),\n",
       " ('L·ªùi k·∫øt n·ªëi',\n",
       "  'A group of men gathered in a supportive community setting, sharing stories and offering encouragement, with a sense of camaraderie and mutual respect. Warm colors, soft lighting, realistic style, --ar 16:9',\n",
       "  'H√¨nh ·∫£nh m·ªôt nh√≥m ƒë√†n √¥ng t·ª• h·ªçp trong m·ªôt c·ªông ƒë·ªìng h·ªó tr·ª£, chia s·∫ª c√¢u chuy·ªán v√† ƒë·ªông vi√™n l·∫´n nhau, th·ªÉ hi·ªán s·ª± g·∫Øn k·∫øt v√† t√¥n tr·ªçng l·∫´n nhau.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_text_image_prompt_generation(\"\"\" \n",
    "üìù B√†i ƒêƒÉng Ng√†y 12 - Th·∫ø H·ªá Di S·∫£n\n",
    "\n",
    "Ng√†y 12: X√¢y D·ª±ng B·∫£n Lƒ©nh - Tr·ªü Th√†nh Phi√™n B·∫£n T·ªët Nh·∫•t C·ªßa Ch√≠nh M√¨nh\n",
    "Qu√Ω √¥ng trong c·ªông ƒë·ªìng Th·∫ø H·ªá Di S·∫£n,\n",
    "H√¥m nay, ch√∫ng ta c√πng b√†n v·ªÅ m·ªôt ch·ªß ƒë·ªÅ c·ªët l√µi: b·∫£n lƒ©nh v√† l√≤ng t·ª± tr·ªçng. Trong th·ªùi ƒë·∫°i bi·∫øn ƒë·ªông n√†y, gi·ªØ v·ªØng ni·ªÅm tin v√†o b·∫£n th√¢n, ƒë·∫∑c bi·ªát v·ªõi ƒë√†n √¥ng, l√† m·ªôt th·ª≠ th√°ch kh√¥ng h·ªÅ nh·ªè.  Nhi·ªÅu ng∆∞·ªùi trong ch√∫ng ta, d√π ·ªü ƒë·ªô tu·ªïi n√†o, ƒë·ªÅu t·ª´ng c·∫£m th·∫•y l·∫°c l√µng, thi·∫øu ƒë·ªãnh h∆∞·ªõng, th·∫≠m ch√≠ nghi ng·ªù ch√≠nh m√¨nh. √Åp l·ª±c c√¥ng vi·ªác, gia ƒë√¨nh, v√† x√£ h·ªôi‚Ä¶ d·ªÖ d√†ng b√†o m√≤n s·ª± t·ª± tin.\n",
    "Nh∆∞ng h√£y nh·ªõ: B·∫£n lƒ©nh l√† n·ªÅn t·∫£ng c·ªßa th√†nh c√¥ng. H√£y tin t∆∞·ªüng v√†o ch√≠nh m√¨nh!  ƒê√¢y kh√¥ng ch·ªâ l√† l·ªùi ƒë·ªông vi√™n, m√† l√† ch√¢n l√Ω c·∫ßn ƒë∆∞·ª£c s·ªëng tr·ªçn v·∫πn.\n",
    "V·∫≠y l√†m th·∫ø n√†o ƒë·ªÉ x√¢y d·ª±ng v√† c·ªßng c·ªë b·∫£n lƒ©nh? ƒê√≥ kh√¥ng ph·∫£i l√† ƒë√≠ch ƒë·∫øn m√† l√† m·ªôt h√†nh tr√¨nh ƒë√≤i h·ªèi s·ª± n·ªó l·ª±c kh√¥ng ng·ª´ng, s·ª± ki√™n tr√¨, v√† nh·∫•t l√† s·ª± t·ª± nh·∫≠n th·ª©c.\n",
    "ƒê·∫ßu ti√™n, h√£y x√°c ƒë·ªãnh gi√° tr·ªã c·ªët l√µi c·ªßa b·∫£n th√¢n.  B·∫°n l√† ai? B·∫°n mu·ªën c·ªëng hi·∫øn g√¨ cho cu·ªôc ƒë·ªùi? Hi·ªÉu r√µ b·∫£n th√¢n l√† b∆∞·ªõc ƒë·ªám v·ªØng ch·∫Øc cho l√≤ng t·ª± tr·ªçng.  \n",
    "Th·ª© hai, h√£y ƒë·∫∑t m·ª•c ti√™u nh·ªè, kh·∫£ thi v√† chinh ph·ª•c t·ª´ng b∆∞·ªõc m·ªôt. M·ªói th√†nh c√¥ng nh·ªè s·∫Ω l√† vi√™n g·∫°ch x√¢y n√™n t√≤a l√¢u ƒë√†i b·∫£n lƒ©nh.  H√£y t∆∞·ªüng t∆∞·ª£ng v·ªÅ m·ªôt ng∆∞·ªùi ƒë√†n √¥ng b·∫°n ng∆∞·ª°ng m·ªô. Anh ·∫•y ƒë√£ l√†m g√¨ ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c th√†nh t·ª±u nh∆∞ ng√†y h√¥m nay? H√£y h·ªçc h·ªèi t·ª´ nh·ªØng t·∫•m g∆∞∆°ng ƒë√≥.\n",
    "V√≠ d·ª•: N·∫øu b·∫°n mu·ªën c·∫£i thi·ªán s·ª©c kh·ªèe, h√£y b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác t·∫≠p th·ªÉ d·ª•c 30 ph√∫t m·ªói ng√†y.  N·∫øu b·∫°n mu·ªën thƒÉng ti·∫øn trong s·ª± nghi·ªáp, h√£y ƒë·∫∑t m·ª•c ti√™u ho√†n th√†nh m·ªôt d·ª± √°n quan tr·ªçng.  Nh·ªØng th√†nh c√¥ng nh·ªè n√†y s·∫Ω t√≠ch l≈©y l·∫°i, t·∫°o n√™n s·ª± t·ª± tin m·∫°nh m·∫Ω.\n",
    "Th·ª© ba, h√£y h·ªçc c√°ch y√™u th∆∞∆°ng v√† ch·∫•p nh·∫≠n b·∫£n th√¢n, c·∫£ ∆∞u ƒëi·ªÉm v√† khuy·∫øt ƒëi·ªÉm.  Sai l·∫ßm l√† ƒëi·ªÅu t·∫•t y·∫øu. H√£y xem ƒë√≥ l√† b√†i h·ªçc qu√Ω gi√° ƒë·ªÉ tr∆∞·ªüng th√†nh.  M·ªôt ng∆∞·ªùi ƒë√†n √¥ng ƒë√≠ch th·ª±c kh√¥ng s·ª£ th·∫•t b·∫°i m√† bi·∫øt ƒë·ª©ng d·∫≠y sau m·ªói v·∫•p ng√£.\n",
    "Cu·ªëi c√πng, h√£y k·∫øt n·ªëi v·ªõi c·ªông ƒë·ªìng.  Chia s·∫ª v·ªõi nh·ªØng ng∆∞·ªùi c√πng ch√≠ h∆∞·ªõng, h·ªçc h·ªèi t·ª´ kinh nghi·ªám c·ªßa h·ªç s·∫Ω gi√∫p b·∫°n v·ªØng v√†ng h∆°n tr√™n con ƒë∆∞·ªùng ho√†n thi·ªán b·∫£n th√¢n. Th·∫ø H·ªá Di S·∫£n ch√≠nh l√† c·ªông ƒë·ªìng m√† b·∫°n c√≥ th·ªÉ t√¨m th·∫•y s·ª± h·ªó tr·ª£ v√† ƒë·ªông l·ª±c ƒë√≥.  \n",
    "H√†nh tr√¨nh r√®n luy·ªán b·∫£n lƒ©nh l√† m·ªôt cu·ªôc chi·∫øn th·∫ßm l·∫∑ng ƒë√≤i h·ªèi s·ª± d≈©ng c·∫£m v√† ki√™n tr√¨.  Nh∆∞ng h√£y nh·ªõ, b·∫°n x·ª©ng ƒë√°ng v·ªõi h·∫°nh ph√∫c v√† th√†nh c√¥ng. üí™\n",
    "Qu√Ω √¥ng c√≥ b√≠ quy·∫øt n√†o ƒë·ªÉ gi·ªØ v·ªØng b·∫£n lƒ©nh trong cu·ªôc s·ªëng? H√£y chia s·∫ª ƒë·ªÉ c√πng nhau x√¢y d·ª±ng m·ªôt c·ªông ƒë·ªìng m·∫°nh m·∫Ω! üëá #TheHeDiSan #BanLƒ©nh #NamT√≠nh #Th√†nhC√¥ng\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
