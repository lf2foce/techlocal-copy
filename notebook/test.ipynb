{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123: A New Approach',\n",
       " \"This theme explores how '123' relates to 123. We‚Äôll break down why this is crucial and how to apply it practically.\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def generate_theme_title_and_story(campaign_title: str, insight: str) -> tuple[str, str]:\n",
    "    title_templates = [\n",
    "        \"The Power of {keyword}\",\n",
    "        \"Mastering {keyword} for Growth\",\n",
    "        \"Why {keyword} Matters Now\",\n",
    "        \"{keyword}: A New Approach\",\n",
    "        \"Unlocking {keyword}\"\n",
    "    ]\n",
    "\n",
    "    insight_keywords = insight.split()\n",
    "    keyword = random.choice(insight_keywords if insight_keywords else [\"change\"])\n",
    "    title = random.choice(title_templates).format(keyword=keyword.capitalize())\n",
    "\n",
    "    story = f\"This theme explores how '{keyword}' relates to {campaign_title.lower()}. We‚Äôll break down why this is crucial and how to apply it practically.\"\n",
    "    return title, story\n",
    "generate_theme_title_and_story(\"123\", \"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def generate_topic():\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a creative assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Generate a unique topic for a technology blog.\"}\n",
    "        ]\n",
    "    )\n",
    "    topic = response.choices[0].message['content']\n",
    "    print(f\"Generated Topic: {topic}\")\n",
    "\n",
    "# Run the asynchronous function\n",
    "asyncio.run(generate_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 posts asynchronously with DeepSeek...\n",
      "Generated post for: Benefits of meditation\n",
      "Generated post for: How to start programming\n",
      "Generated post for: Climate change solutions\n",
      "Generated post for: Healthy meal prep ideas\n",
      "Generated post for: Financial planning tips\n",
      "Generated post for: Productivity hacks\n",
      "Generated post for: Travel on a budget\n",
      "Generated post for: Home workout routines\n",
      "Generated post for: Book recommendations for beginners\n",
      "Generated post for: Tech trends 2025\n",
      "\n",
      "Generated posts summary:\n",
      "1. Benefits of meditation - 1359 chars\n",
      "2. How to start programming - 1472 chars\n",
      "3. Climate change solutions - 1494 chars\n",
      "4. Healthy meal prep ideas - 1511 chars\n",
      "5. Financial planning tips - 1394 chars\n",
      "6. Productivity hacks - 1185 chars\n",
      "7. Travel on a budget - 1454 chars\n",
      "8. Home workout routines - 1395 chars\n",
      "9. Book recommendations for beginners - 1418 chars\n",
      "10. Tech trends 2025 - 1705 chars\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ab313/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ab313/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Export to Excel\u001b[39;00m\n\u001b[32m     83\u001b[39m excel_filename = \u001b[33m\"\u001b[39m\u001b[33mdeepseek_generated_posts.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m end_time = time.time()\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m posts to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexcel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/core/generic.py:2417\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2406\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2407\u001b[39m     df,\n\u001b[32m   2408\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2415\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2416\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sites/rewrite_copy2/.venv/lib/python3.13/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load environment variables and apply nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenAI client with DeepSeek endpoint\n",
    "client = OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Define the topics for the posts\n",
    "topics = [\n",
    "    \"Benefits of meditation\",\n",
    "    \"How to start programming\",\n",
    "    \"Climate change solutions\",\n",
    "    \"Healthy meal prep ideas\",\n",
    "    \"Financial planning tips\",\n",
    "    \"Productivity hacks\",\n",
    "    \"Travel on a budget\",\n",
    "    \"Home workout routines\",\n",
    "    \"Book recommendations for beginners\",\n",
    "    \"Tech trends 2025\"\n",
    "]\n",
    "\n",
    "async def generate_post(topic):\n",
    "    \"\"\"Generate a post about a specific topic using DeepSeek.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes blog posts\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Write a short blog post about {topic}. Keep it under 300 words.\"},\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"Generated post for: {topic}\")\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating post for {topic}: {e}\")\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"content\": f\"Error: {str(e)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "async def process_topics():\n",
    "    \"\"\"Process all topics concurrently.\"\"\"\n",
    "    tasks = []\n",
    "    for topic in topics:\n",
    "        task = asyncio.create_task(generate_post(topic))\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "async def main():\n",
    "    print(\"Generating 10 posts asynchronously with DeepSeek...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = await process_topics()\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display summary of the DataFrame\n",
    "    print(\"\\nGenerated posts summary:\")\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"{index+1}. {row['topic']} - {len(row['content'])} chars\")\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_filename = \"deepseek_generated_posts.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExported {len(results)} posts to {excel_filename}\")\n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating posts asynchronously with Google Gemini...\n",
      "Generated post for: Benefits of meditation\n",
      "Generated post for: How to start programming\n",
      "Generated post for: Climate change solutions\n",
      "Generated post for: Healthy meal prep ideas\n",
      "Generated post for: Financial planning tips\n",
      "Generated post for: Productivity hacks\n",
      "Generated post for: Travel on a budget\n",
      "Generated post for: Home workout routines\n",
      "Generated post for: Book recommendations for beginners\n",
      "Generated post for: Tech trends 2025\n",
      "\n",
      "Generated posts summary:\n",
      "1. Unlock Inner Peace: The Benefits of Meditation - 1348 chars\n",
      "2. Embark on Your Coding Journey: A Beginner's Guide - 1076 chars\n",
      "3. Fighting Back: Practical Climate Change Solutions - 1221 chars\n",
      "4. Fuel Your Week: Easy & Healthy Meal Prep Ideas - 1209 chars\n",
      "5. Smart Financial Planning Tips for a Secure Future - 791 chars\n",
      "6. Boost Your Day: Simple Productivity Hacks - 900 chars\n",
      "7. Travel on a Budget: See the World Without Breaking the Bank - 1239 chars\n",
      "8. Effective Home Workout Routines - 1158 chars\n",
      "9. Page-Turners for Budding Bookworms: Great Reads to Get You Hooked - 1119 chars\n",
      "10. Tech Trends to Watch in 2025 - 1070 chars\n",
      "\n",
      "Exported 10 posts to gemini_generated_posts.xlsx\n",
      "Total time: 25.88 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# Load environment variables and apply nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define topics for generation\n",
    "topics = [\n",
    "    \"Benefits of meditation\",\n",
    "    \"How to start programming\",\n",
    "    \"Climate change solutions\",\n",
    "    \"Healthy meal prep ideas\",\n",
    "    \"Financial planning tips\",\n",
    "    \"Productivity hacks\",\n",
    "    \"Travel on a budget\",\n",
    "    \"Home workout routines\",\n",
    "    \"Book recommendations for beginners\",\n",
    "    \"Tech trends 2025\"\n",
    "]\n",
    "\n",
    "\n",
    "# System prompt - adapt this to your needs\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that writes engaging blog posts. \n",
    "Create content that is informative, well-structured, and under 300 words.\n",
    "Format your response as a JSON object with 'title' and 'content' fields.\n",
    "\"\"\"\n",
    "\n",
    "class BlogPost(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "async def generate_post_with_gemini(topic: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a post about a specific topic using Google Gemini.\"\"\"\n",
    "    try:\n",
    "        # Initialize the client similar to your implementation\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\") )\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"Write a short blog post about {topic}. Keep it under 300 words.\"\n",
    "        \n",
    "        # Generate response using Gemini API - following your implementation pattern\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'response_mime_type': 'application/json',\n",
    "                'response_schema': BlogPost,\n",
    "                'system_instruction': types.Part.from_text(text=system_prompt),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # Extract the response - similar to your approach\n",
    "        print(f\"Generated post for: {topic}\")\n",
    "        content = json.loads(response.text)\n",
    "        blog_post = BlogPost(**content)\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"title\": blog_post.title,\n",
    "            \"content\": blog_post.content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Gemini API Error: {str(e)}\")\n",
    "\n",
    "\n",
    "# This function handles creating the semaphore and processing tasks\n",
    "async def process_with_semaphore(topics: List[str], concurrency: int = 10):\n",
    "    \"\"\"Process topics with a semaphore to limit concurrent API calls.\"\"\"\n",
    "    # Use a semaphore to control concurrency\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "    \n",
    "    async def bounded_generate(topic):\n",
    "        async with semaphore:\n",
    "            return await generate_post_with_gemini(topic)\n",
    "    \n",
    "    tasks = [bounded_generate(topic) for topic in topics]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    print(\"Generating posts asynchronously with Google Gemini...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process all topics with controlled concurrency\n",
    "    results = await process_with_semaphore(topics)\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display summary of the DataFrame\n",
    "    print(\"\\nGenerated posts summary:\")\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"{index+1}. {row['title']} - {len(row['content'])} chars\")\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_filename = \"gemini_generated_posts.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExported {len(results)} posts to {excel_filename}\")\n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated post based on user prompt.\n",
      "Title: Unlocking Inner Peace: The Benefits of Meditation\n",
      "Content: In today's fast-paced world, finding moments of calm can feel like a luxury. However, meditation offers a simple yet powerful way to cultivate inner peace and improve overall well-being. Regular meditation practice provides numerous benefits for both your mind and body.\n",
      "\n",
      "One of the most significant advantages is stress reduction. Meditation helps calm the nervous system, lowering cortisol levels and promoting relaxation. This can lead to reduced anxiety and a greater sense of emotional balance. Beyond stress relief, meditation enhances focus and concentration. By training your mind to stay present, you can improve your ability to concentrate on tasks and make better decisions.\n",
      "\n",
      "Furthermore, meditation can positively impact your physical health. Studies have shown that it can lower blood pressure, improve sleep quality, and even boost your immune system. It also promotes self-awareness, allowing you to better understand your thoughts, feelings, and behaviors. This increased self-knowledge can lead to healthier relationships and a greater sense of purpose.\n",
      "\n",
      "Whether you're a beginner or an experienced practitioner, incorporating even a few minutes of daily meditation into your routine can make a profound difference in your life.\n",
      "Timestamp: 2025-04-04 01:15:44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# System prompt - adapt this to your needs\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that writes engaging blog posts. \n",
    "Create content that is informative, well-structured, and under 300 words.\n",
    "Format your response as a JSON object with 'title' and 'content' fields.\n",
    "\"\"\"\n",
    "\n",
    "class BlogPost(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "async def generate_post_with_gemini(user_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a post based on a user-provided prompt using Google Gemini.\"\"\"\n",
    "    try:\n",
    "        # Initialize the client\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        \n",
    "        # Generate response using Gemini API\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=user_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_mime_type='application/json',\n",
    "                response_schema=BlogPost,\n",
    "                system_instruction=types.Part.from_text(text=system_prompt),\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        print(\"Generated post based on user prompt.\")\n",
    "        content = json.loads(response.text)\n",
    "        \n",
    "        # Validate and parse the response using Pydantic\n",
    "        blog_post = BlogPost(**content)\n",
    "        \n",
    "        return {\n",
    "            \"title\": blog_post.title,\n",
    "            \"content\": blog_post.content,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except ValidationError as ve:\n",
    "        print(f\"Validation error: {ve}\")\n",
    "        return {\n",
    "            \"title\": \"Validation Error\",\n",
    "            \"content\": f\"Validation Error: {str(ve)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating post: {e}\")\n",
    "        return {\n",
    "            \"title\": \"Error\",\n",
    "            \"content\": f\"Error: {str(e)}\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "user_prompt = \"Write a short blog post about the benefits of meditation. Keep it under 300 words.\"\n",
    "\n",
    "# Run the asynchronous function\n",
    "result = asyncio.run(generate_post_with_gemini(user_prompt))\n",
    "\n",
    "# Display the result\n",
    "print(\"Title:\", result[\"title\"])\n",
    "print(\"Content:\", result[\"content\"])\n",
    "print(\"Timestamp:\", result[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_image_prompts_for_post(post_content_vn: str, campaign_knowledge: str, brand_name: str, day_num: int):\n",
    "    print(Fore.CYAN + f\"\\nB∆∞·ªõc 10: T·∫°o c√°c prompt ·∫£nh theo m·∫°ch truy·ªán cho b√†i ƒëƒÉng Ng√†y {day_num}...\")\n",
    "    MAX_POST_LENGTH_FOR_PROMPT = 2500\n",
    "    truncated_content = post_content_vn[:MAX_POST_LENGTH_FOR_PROMPT]\n",
    "    if len(post_content_vn) > MAX_POST_LENGTH_FOR_PROMPT:\n",
    "         print(Fore.YELLOW + f\"    (N·ªôi dung b√†i vi·∫øt r√∫t g·ªçn c√≤n {MAX_POST_LENGTH_FOR_PROMPT} k√Ω t·ª± ƒë·ªÉ t·∫°o prompt ·∫£nh)\")\n",
    "    prompt = (\n",
    "        \"V·ªõi vai tr√≤ l√† chuy√™n gia visual storytelling, h√£y ph√¢n t√≠ch b√†i ƒëƒÉng ti·∫øng Vi·ªát d∆∞·ªõi ƒë√¢y.\\n\"\n",
    "        \"D·ª±a tr√™n t√™n th∆∞∆°ng hi·ªáu, m√¥ t·∫£ k√™nh v√† n·ªôi dung b√†i ƒëƒÉng, t·∫°o ra **3 ƒë·∫øn 4 c·∫∑p prompt ·∫£nh** minh h·ªça c√°c ph·∫ßn ch√≠nh/c·∫£m x√∫c quan tr·ªçng.\\n\\n\"\n",
    "        \"M·ªói c·∫∑p prompt g·ªìm:\\n\"\n",
    "        \"1. **`part` (string)**: Nh√£n ti·∫øng Vi·ªát ng·∫Øn g·ªçn x√°c ƒë·ªãnh ph·∫ßn minh h·ªça (VD: 'M·ªü ƒë·∫ßu', 'N·ªói trƒÉn tr·ªü', 'ƒêi·ªÉm s√°ng', 'Th√¥ng ƒëi·ªáp ch√≠nh', 'L·ªùi k·∫øt n·ªëi', 'H√†nh ƒë·ªông').\\n\"\n",
    "        \"2. **`english_prompt` (string)**: Prompt **ti·∫øng Anh** chi ti·∫øt (ch·ªß th·ªÉ, h√†nh ƒë·ªông, b·ªëi c·∫£nh, √°nh s√°ng, m√†u s·∫Øc, mood, phong c√°ch [cinematic, realistic, illustration, symbolic...], t·ª∑ l·ªá [--ar 16:9, --ar 1:1]).\\n\"\n",
    "        \"3. **`vietnamese_explanation` (string)**: Gi·∫£i th√≠ch **ti·∫øng Vi·ªát** ng·∫Øn g·ªçn l√Ω do prompt ph√π h·ª£p.\\n\\n\"\n",
    "        f\"--- T√äN TH∆Ø∆†NG HI·ªÜU ---\\n{brand_name}\\n\\n\"\n",
    "        f\"--- M√î T·∫¢ K√äNH ---\\n{campaign_knowledge}\\n--- END KNOWLEDGE ---\\n\\n\"\n",
    "        f\"--- B√ÄI ƒêƒÇNG TI·∫æNG VI·ªÜT (Ng√†y {day_num}) ---\\n'''{truncated_content}'''\\n--- END POST ---\\n\\n\"\n",
    "        \"Y√™u c·∫ßu Output:\\n\"\n",
    "        \"- Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\\n\"\n",
    "        \"- Value c·ªßa 'story_prompts' l√† list ch·ª©a 3 ho·∫∑c 4 objects (∆∞u ti√™n 4 n·∫øu ƒë·ªß √Ω).\\n\"\n",
    "        \"- M·ªói object c√≥ 3 string keys: 'part', 'english_prompt', 'vietnamese_explanation', ƒë·ªÅu kh√¥ng r·ªóng.\"\n",
    "    )\n",
    "    response_data = gemini_text_generation(prompt, response_schema=True)\n",
    "\n",
    "    if not isinstance(response_data, dict) or 'story_prompts' not in response_data:\n",
    "        print(Fore.RED + f\"    L·ªói: ƒê·ªãnh d·∫°ng JSON prompt truy·ªán kh√¥ng ƒë√∫ng (thi·∫øu 'story_prompts'). Data: {response_data}\")\n",
    "        return None\n",
    "    prompts_list = response_data['story_prompts']\n",
    "    MIN_PROMPTS = 2\n",
    "    if not isinstance(prompts_list, list) or len(prompts_list) < MIN_PROMPTS:\n",
    "        print(Fore.RED + f\"    L·ªói: 'story_prompts' kh√¥ng ph·∫£i list ho·∫∑c < {MIN_PROMPTS} ph·∫ßn t·ª≠ (c√≥ {len(prompts_list) if isinstance(prompts_list, list) else 'N/A'}). Data: {response_data}\")\n",
    "        return None\n",
    "\n",
    "    valid_prompts = []\n",
    "    for i, item in enumerate(prompts_list):\n",
    "        if not (isinstance(item, dict) and\n",
    "                all(k in item for k in ['part', 'english_prompt', 'vietnamese_explanation']) and\n",
    "                isinstance(item.get('part'), str) and item.get('part').strip() and\n",
    "                isinstance(item.get('english_prompt'), str) and item.get('english_prompt').strip() and\n",
    "                isinstance(item.get('vietnamese_explanation'), str) and item.get('vietnamese_explanation').strip()):\n",
    "            print(Fore.YELLOW + f\"    C·∫£nh b√°o: Item prompt truy·ªán th·ª© {i+1} kh√¥ng h·ª£p l·ªá, b·ªè qua: {item}\")\n",
    "            continue\n",
    "        else:\n",
    "            valid_prompts.append({\n",
    "                'part': item['part'].strip(),\n",
    "                'english_prompt': item['english_prompt'].strip(),\n",
    "                'vietnamese_explanation': item['vietnamese_explanation'].strip()\n",
    "            })\n",
    "\n",
    "    if len(valid_prompts) < MIN_PROMPTS:\n",
    "         print(Fore.RED + f\"    L·ªói: Sau khi l·ªçc, ch·ªâ c√≤n {len(valid_prompts)} prompt truy·ªán h·ª£p l·ªá (< {MIN_PROMPTS}). Data: {response_data}\")\n",
    "         return None\n",
    "\n",
    "    print(Fore.GREEN + f\"    ƒê√£ t·∫°o v√† x√°c th·ª±c {len(valid_prompts)} prompt ·∫£nh theo m·∫°ch truy·ªán cho Ng√†y {day_num} th√†nh c√¥ng.\")\n",
    "    return valid_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_image_prompts_for_post(post_content_vn: str, campaign_knowledge: str, brand_name: str, day_num: int):\n",
    "    print(f\"\\nB∆∞·ªõc 10: T·∫°o prompt ·∫£nh cho b√†i ƒëƒÉng Ng√†y {day_num}...\")\n",
    "\n",
    "    truncated = post_content_vn[:2500]\n",
    "    if len(post_content_vn) > 2500:\n",
    "        print(\"    (R√∫t g·ªçn b√†i vi·∫øt xu·ªëng 2500 k√Ω t·ª± ƒë·ªÉ t·∫°o prompt)\")\n",
    "\n",
    "    prompt = (\n",
    "        \"V·ªõi vai tr√≤ l√† chuy√™n gia visual storytelling, h√£y ph√¢n t√≠ch b√†i ƒëƒÉng ti·∫øng Vi·ªát d∆∞·ªõi ƒë√¢y.\\n\"\n",
    "        \"D·ª±a tr√™n t√™n th∆∞∆°ng hi·ªáu, m√¥ t·∫£ k√™nh v√† n·ªôi dung b√†i ƒëƒÉng, t·∫°o ra **3 ƒë·∫øn 4 c·∫∑p prompt ·∫£nh** minh h·ªça c√°c ph·∫ßn ch√≠nh/c·∫£m x√∫c quan tr·ªçng.\\n\\n\"\n",
    "        \"M·ªói c·∫∑p prompt g·ªìm:\\n\"\n",
    "        \"1. **`part` (string)**: Nh√£n ti·∫øng Vi·ªát ng·∫Øn g·ªçn x√°c ƒë·ªãnh ph·∫ßn minh h·ªça (VD: 'M·ªü ƒë·∫ßu', 'N·ªói trƒÉn tr·ªü', 'ƒêi·ªÉm s√°ng', 'Th√¥ng ƒëi·ªáp ch√≠nh', 'L·ªùi k·∫øt n·ªëi', 'H√†nh ƒë·ªông').\\n\"\n",
    "        \"2. **`english_prompt` (string)**: Prompt **ti·∫øng Anh** chi ti·∫øt (ch·ªß th·ªÉ, h√†nh ƒë·ªông, b·ªëi c·∫£nh, √°nh s√°ng, m√†u s·∫Øc, mood, phong c√°ch [cinematic, realistic, illustration, symbolic...], t·ª∑ l·ªá [--ar 16:9, --ar 1:1]).\\n\"\n",
    "        \"3. **`vietnamese_explanation` (string)**: Gi·∫£i th√≠ch **ti·∫øng Vi·ªát** ng·∫Øn g·ªçn l√Ω do prompt ph√π h·ª£p.\\n\\n\"\n",
    "        f\"--- T√äN TH∆Ø∆†NG HI·ªÜU ---\\n{brand_name}\\n\\n\"\n",
    "        f\"--- M√î T·∫¢ K√äNH ---\\n{campaign_knowledge}\\n--- END KNOWLEDGE ---\\n\\n\"\n",
    "        f\"--- B√ÄI ƒêƒÇNG TI·∫æNG VI·ªÜT (Ng√†y {day_num}) ---\\n'''{truncated_content}'''\\n--- END POST ---\\n\\n\"\n",
    "        \"Y√™u c·∫ßu Output:\\n\"\n",
    "        \"- Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\\n\"\n",
    "        \"- Value c·ªßa 'story_prompts' l√† list ch·ª©a 3 ho·∫∑c 4 objects (∆∞u ti√™n 4 n·∫øu ƒë·ªß √Ω).\\n\"\n",
    "        \"- M·ªói object c√≥ 3 string keys: 'part', 'english_prompt', 'vietnamese_e  xplanation', ƒë·ªÅu kh√¥ng r·ªóng.\"\n",
    "    )\n",
    "\n",
    "    response = gemini_text_generation(prompt, response_schema=True)\n",
    "\n",
    "    if not isinstance(response, dict) or 'story_prompts' not in response:\n",
    "        print(\"    ‚ùå L·ªói: Thi·∫øu 'story_prompts' trong JSON.\")\n",
    "        return None\n",
    "\n",
    "    prompts = response['story_prompts']\n",
    "    if not isinstance(prompts, list) or len(prompts) < 2:\n",
    "        print(f\"    ‚ùå L·ªói: Ch·ªâ c√≥ {len(prompts) if isinstance(prompts, list) else 'N/A'} prompt.\")\n",
    "        return None\n",
    "\n",
    "    valid = []\n",
    "    for i, p in enumerate(prompts):\n",
    "        if all(isinstance(p.get(k), str) and p[k].strip() for k in ['part', 'english_prompt', 'vietnamese_explanation']):\n",
    "            valid.append({k: p[k].strip() for k in ['part', 'english_prompt', 'vietnamese_explanation']})\n",
    "        else:\n",
    "            print(f\"    ‚ö†Ô∏è Prompt {i+1} kh√¥ng h·ª£p l·ªá, b·ªè qua.\")\n",
    "\n",
    "    if len(valid) < 2:\n",
    "        print(f\"    ‚ùå L·ªói: Ch·ªâ c√≤n {len(valid)} prompt h·ª£p l·ªá sau khi l·ªçc.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"    ‚úÖ T·∫°o {len(valid)} prompt ·∫£nh th√†nh c√¥ng.\")\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "import os, json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ImagePrompt(BaseModel):\n",
    "    part: str\n",
    "    english_prompt: str\n",
    "    vietnamese_explanation: str\n",
    "\n",
    "class ImagePromptGenerate(BaseModel):\n",
    "    story_prompts: List[ImagePrompt]\n",
    "\n",
    "def gemini_text_image_prompt_generation(text: str):\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    \n",
    "    system_prompt= \"\"\" \n",
    "    V·ªõi vai tr√≤ l√† chuy√™n gia visual storytelling, h√£y ph√¢n t√≠ch b√†i ƒëƒÉng ti·∫øng Vi·ªát d∆∞·ªõi ƒë√¢y.\n",
    "    D·ª±a tr√™n t√™n th∆∞∆°ng hi·ªáu, m√¥ t·∫£ k√™nh v√† n·ªôi dung b√†i ƒëƒÉng, t·∫°o ra **4 c·∫∑p prompt ·∫£nh** minh h·ªça c√°c ph·∫ßn ch√≠nh/c·∫£m x√∫c quan tr·ªçng.\n",
    "    M·ªói c·∫∑p prompt g·ªìm:\n",
    "    1. **`part` (string)**: Nh√£n ti·∫øng Vi·ªát ng·∫Øn g·ªçn x√°c ƒë·ªãnh ph·∫ßn minh h·ªça (VD: 'M·ªü ƒë·∫ßu', 'N·ªói trƒÉn tr·ªü', 'ƒêi·ªÉm s√°ng', 'Th√¥ng ƒëi·ªáp ch√≠nh', 'L·ªùi k·∫øt n·ªëi', 'H√†nh ƒë·ªông').\n",
    "    2. **`english_prompt` (string)**: Prompt **ti·∫øng Anh** chi ti·∫øt (ch·ªß th·ªÉ, h√†nh ƒë·ªông, b·ªëi c·∫£nh, √°nh s√°ng, m√†u s·∫Øc, mood, phong c√°ch [cinematic, realistic, illustration, symbolic...], t·ª∑ l·ªá [--ar 16:9, --ar 1:1]).\n",
    "    3. **`vietnamese_explanation` (string)**: Gi·∫£i th√≠ch **ti·∫øng Vi·ªát** ng·∫Øn g·ªçn l√Ω do prompt ph√π h·ª£p.\n",
    "\n",
    "    Y√™u c·∫ßu Output:\n",
    "    - Return ONLY a valid JSON object (no surrounding text/markdown) with a single key 'story_prompts'.\n",
    "    - Value c·ªßa 'story_prompts' l√† list ch·ª©a 4 objects (∆∞u ti√™n 4 n·∫øu ƒë·ªß √Ω).\n",
    "    - M·ªói object c√≥ 3 string keys: 'part', 'english_prompt', 'vietnamese_explanation', ƒë·ªÅu kh√¥ng r·ªóng.\n",
    "    \"\"\"\n",
    "    # Generate response using Gemini API (synchronous version)\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=f\"Tr·∫£ th√¥ng tin cho n·ªôi dung sau ƒë√¢y: {text}\", # T·∫°o 5 th∆∞∆°ng hi·ªáu cho pages v·ªõi c√°c th√¥ng tin {insight} {target_customer}. M·ªói th∆∞∆°ng hi·ªáu ph·∫£i c√≥ title v√† story kh√°c nhau.\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type='application/json',\n",
    "            response_schema=ImagePromptGenerate,\n",
    "            system_instruction=types.Part.from_text(text=system_prompt),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Extract the response\n",
    "    print(\"Generated 5 themes based on user prompt.\")\n",
    "    content = json.loads(response.text)\n",
    "    \n",
    "    # Validate and parse the response using Pydantic\n",
    "    prompts = ImagePromptGenerate(**content)\n",
    "    \n",
    "    return [(prompt.part, prompt.english_prompt, prompt.vietnamese_explanation) for prompt  in prompts.story_prompts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 themes based on user prompt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('M·ªü ƒë·∫ßu',\n",
       "  'A cinematic shot of a lone man silhouetted against a stormy sky, symbolizing inner turmoil and the challenges of maintaining self-belief in a turbulent world. Moody lighting, desaturated colors, --ar 16:9',\n",
       "  'H√¨nh ·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒë∆°n ƒë·ªôc ƒë·ªëi di·ªán v·ªõi b·∫ßu tr·ªùi gi√¥ng b√£o th·ªÉ hi·ªán s·ª± l·∫°c l√µng v√† th·ª≠ th√°ch trong vi·ªác gi·ªØ v·ªØng ni·ªÅm tin v√†o b·∫£n th√¢n.'),\n",
       " ('Gi√° tr·ªã c·ªët l√µi',\n",
       "  'An illustration of a man standing at a crossroads, contemplating different paths, with symbolic representations of values like integrity, kindness, and resilience glowing softly in the background. Warm, inviting colors, illustration style, --ar 1:1',\n",
       "  'H√¨nh ·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒë·ª©ng tr∆∞·ªõc ng√£ r·∫Ω cu·ªôc ƒë·ªùi, suy ng·∫´m v·ªÅ c√°c gi√° tr·ªã c·ªët l√µi nh∆∞ s·ª± ch√≠nh tr·ª±c, l√≤ng t·ªët v√† kh·∫£ nƒÉng ph·ª•c h·ªìi.'),\n",
       " ('H√†nh ƒë·ªông nh·ªè',\n",
       "  'A realistic image of a man diligently working on a small project, with sunlight streaming through the window, highlighting the sense of accomplishment and progress. Realistic style, warm lighting, --ar 16:9',\n",
       "  'H√¨nh ·∫£nh ng∆∞·ªùi ƒë√†n √¥ng t·ªâ m·ªâ th·ª±c hi·ªán m·ªôt d·ª± √°n nh·ªè, √°nh n·∫Øng chi·∫øu v√†o th·ªÉ hi·ªán c·∫£m gi√°c th√†nh t·ª±u v√† ti·∫øn b·ªô.'),\n",
       " ('L·ªùi k·∫øt n·ªëi',\n",
       "  'A group of men gathered in a supportive community setting, sharing stories and offering encouragement, with a sense of camaraderie and mutual respect. Warm colors, soft lighting, realistic style, --ar 16:9',\n",
       "  'H√¨nh ·∫£nh m·ªôt nh√≥m ƒë√†n √¥ng t·ª• h·ªçp trong m·ªôt c·ªông ƒë·ªìng h·ªó tr·ª£, chia s·∫ª c√¢u chuy·ªán v√† ƒë·ªông vi√™n l·∫´n nhau, th·ªÉ hi·ªán s·ª± g·∫Øn k·∫øt v√† t√¥n tr·ªçng l·∫´n nhau.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_text_image_prompt_generation(\"\"\" \n",
    "üìù B√†i ƒêƒÉng Ng√†y 12 - Th·∫ø H·ªá Di S·∫£n\n",
    "\n",
    "Ng√†y 12: X√¢y D·ª±ng B·∫£n Lƒ©nh - Tr·ªü Th√†nh Phi√™n B·∫£n T·ªët Nh·∫•t C·ªßa Ch√≠nh M√¨nh\n",
    "Qu√Ω √¥ng trong c·ªông ƒë·ªìng Th·∫ø H·ªá Di S·∫£n,\n",
    "H√¥m nay, ch√∫ng ta c√πng b√†n v·ªÅ m·ªôt ch·ªß ƒë·ªÅ c·ªët l√µi: b·∫£n lƒ©nh v√† l√≤ng t·ª± tr·ªçng. Trong th·ªùi ƒë·∫°i bi·∫øn ƒë·ªông n√†y, gi·ªØ v·ªØng ni·ªÅm tin v√†o b·∫£n th√¢n, ƒë·∫∑c bi·ªát v·ªõi ƒë√†n √¥ng, l√† m·ªôt th·ª≠ th√°ch kh√¥ng h·ªÅ nh·ªè.  Nhi·ªÅu ng∆∞·ªùi trong ch√∫ng ta, d√π ·ªü ƒë·ªô tu·ªïi n√†o, ƒë·ªÅu t·ª´ng c·∫£m th·∫•y l·∫°c l√µng, thi·∫øu ƒë·ªãnh h∆∞·ªõng, th·∫≠m ch√≠ nghi ng·ªù ch√≠nh m√¨nh. √Åp l·ª±c c√¥ng vi·ªác, gia ƒë√¨nh, v√† x√£ h·ªôi‚Ä¶ d·ªÖ d√†ng b√†o m√≤n s·ª± t·ª± tin.\n",
    "Nh∆∞ng h√£y nh·ªõ: B·∫£n lƒ©nh l√† n·ªÅn t·∫£ng c·ªßa th√†nh c√¥ng. H√£y tin t∆∞·ªüng v√†o ch√≠nh m√¨nh!  ƒê√¢y kh√¥ng ch·ªâ l√† l·ªùi ƒë·ªông vi√™n, m√† l√† ch√¢n l√Ω c·∫ßn ƒë∆∞·ª£c s·ªëng tr·ªçn v·∫πn.\n",
    "V·∫≠y l√†m th·∫ø n√†o ƒë·ªÉ x√¢y d·ª±ng v√† c·ªßng c·ªë b·∫£n lƒ©nh? ƒê√≥ kh√¥ng ph·∫£i l√† ƒë√≠ch ƒë·∫øn m√† l√† m·ªôt h√†nh tr√¨nh ƒë√≤i h·ªèi s·ª± n·ªó l·ª±c kh√¥ng ng·ª´ng, s·ª± ki√™n tr√¨, v√† nh·∫•t l√† s·ª± t·ª± nh·∫≠n th·ª©c.\n",
    "ƒê·∫ßu ti√™n, h√£y x√°c ƒë·ªãnh gi√° tr·ªã c·ªët l√µi c·ªßa b·∫£n th√¢n.  B·∫°n l√† ai? B·∫°n mu·ªën c·ªëng hi·∫øn g√¨ cho cu·ªôc ƒë·ªùi? Hi·ªÉu r√µ b·∫£n th√¢n l√† b∆∞·ªõc ƒë·ªám v·ªØng ch·∫Øc cho l√≤ng t·ª± tr·ªçng.  \n",
    "Th·ª© hai, h√£y ƒë·∫∑t m·ª•c ti√™u nh·ªè, kh·∫£ thi v√† chinh ph·ª•c t·ª´ng b∆∞·ªõc m·ªôt. M·ªói th√†nh c√¥ng nh·ªè s·∫Ω l√† vi√™n g·∫°ch x√¢y n√™n t√≤a l√¢u ƒë√†i b·∫£n lƒ©nh.  H√£y t∆∞·ªüng t∆∞·ª£ng v·ªÅ m·ªôt ng∆∞·ªùi ƒë√†n √¥ng b·∫°n ng∆∞·ª°ng m·ªô. Anh ·∫•y ƒë√£ l√†m g√¨ ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c th√†nh t·ª±u nh∆∞ ng√†y h√¥m nay? H√£y h·ªçc h·ªèi t·ª´ nh·ªØng t·∫•m g∆∞∆°ng ƒë√≥.\n",
    "V√≠ d·ª•: N·∫øu b·∫°n mu·ªën c·∫£i thi·ªán s·ª©c kh·ªèe, h√£y b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác t·∫≠p th·ªÉ d·ª•c 30 ph√∫t m·ªói ng√†y.  N·∫øu b·∫°n mu·ªën thƒÉng ti·∫øn trong s·ª± nghi·ªáp, h√£y ƒë·∫∑t m·ª•c ti√™u ho√†n th√†nh m·ªôt d·ª± √°n quan tr·ªçng.  Nh·ªØng th√†nh c√¥ng nh·ªè n√†y s·∫Ω t√≠ch l≈©y l·∫°i, t·∫°o n√™n s·ª± t·ª± tin m·∫°nh m·∫Ω.\n",
    "Th·ª© ba, h√£y h·ªçc c√°ch y√™u th∆∞∆°ng v√† ch·∫•p nh·∫≠n b·∫£n th√¢n, c·∫£ ∆∞u ƒëi·ªÉm v√† khuy·∫øt ƒëi·ªÉm.  Sai l·∫ßm l√† ƒëi·ªÅu t·∫•t y·∫øu. H√£y xem ƒë√≥ l√† b√†i h·ªçc qu√Ω gi√° ƒë·ªÉ tr∆∞·ªüng th√†nh.  M·ªôt ng∆∞·ªùi ƒë√†n √¥ng ƒë√≠ch th·ª±c kh√¥ng s·ª£ th·∫•t b·∫°i m√† bi·∫øt ƒë·ª©ng d·∫≠y sau m·ªói v·∫•p ng√£.\n",
    "Cu·ªëi c√πng, h√£y k·∫øt n·ªëi v·ªõi c·ªông ƒë·ªìng.  Chia s·∫ª v·ªõi nh·ªØng ng∆∞·ªùi c√πng ch√≠ h∆∞·ªõng, h·ªçc h·ªèi t·ª´ kinh nghi·ªám c·ªßa h·ªç s·∫Ω gi√∫p b·∫°n v·ªØng v√†ng h∆°n tr√™n con ƒë∆∞·ªùng ho√†n thi·ªán b·∫£n th√¢n. Th·∫ø H·ªá Di S·∫£n ch√≠nh l√† c·ªông ƒë·ªìng m√† b·∫°n c√≥ th·ªÉ t√¨m th·∫•y s·ª± h·ªó tr·ª£ v√† ƒë·ªông l·ª±c ƒë√≥.  \n",
    "H√†nh tr√¨nh r√®n luy·ªán b·∫£n lƒ©nh l√† m·ªôt cu·ªôc chi·∫øn th·∫ßm l·∫∑ng ƒë√≤i h·ªèi s·ª± d≈©ng c·∫£m v√† ki√™n tr√¨.  Nh∆∞ng h√£y nh·ªõ, b·∫°n x·ª©ng ƒë√°ng v·ªõi h·∫°nh ph√∫c v√† th√†nh c√¥ng. üí™\n",
    "Qu√Ω √¥ng c√≥ b√≠ quy·∫øt n√†o ƒë·ªÉ gi·ªØ v·ªØng b·∫£n lƒ©nh trong cu·ªôc s·ªëng? H√£y chia s·∫ª ƒë·ªÉ c√πng nhau x√¢y d·ª±ng m·ªôt c·ªông ƒë·ªìng m·∫°nh m·∫Ω! üëá #TheHeDiSan #BanLƒ©nh #NamT√≠nh #Th√†nhC√¥ng\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 {\"type\": \"about:blank\", \"title\": \"Bad Request\", \"detail\": \"'9:16' is not one of ['ASPECT_10_16', 'ASPECT_16_10', 'ASPECT_9_16', 'ASPECT_16_9', 'ASPECT_3_2', 'ASPECT_2_3', 'ASPECT_4_3', 'ASPECT_3_4', 'ASPECT_1_1', 'ASPECT_1_3', 'ASPECT_3_1'] - 'image_request.aspect_ratio'\", \"status\": 400}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import asyncio\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "load_dotenv()\n",
    "url = \"https://api.ideogram.ai/generate\"\n",
    "headers = {\n",
    "    \"Api-Key\": os.getenv(\"IDEO_API_KEY\"),\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"image_request\": {\n",
    "        \"prompt\": \"A serene tropical beach scene with palm trees and a lighthouse at sunset\",\n",
    "        \"aspect_ratio\": \"ASPECT_10_16\",\n",
    "        \"model\": \"V_2\",\n",
    "        \"magic_prompt_option\": \"AUTO\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"Image URL:\", data[\"data\"][0][\"url\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your original service account file\n",
    "input_path = \"service_account.json\"  # or wherever you saved it\n",
    "\n",
    "# Load the JSON\n",
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Escape the private_key field (main issue with Render envs)\n",
    "if \"private_key\" in data:\n",
    "    data[\"private_key\"] = data[\"private_key\"].replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "# Convert full JSON to single-line safe string\n",
    "render_safe_json = json.dumps(data)\n",
    "\n",
    "# Print it out for you to copy-paste into Render\n",
    "# print(\"\\n‚úÖ COPY THIS INTO RENDER SECRET FILE:\\n\")\n",
    "# print(render_safe_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtogether\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Together\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m Together()\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma flying cat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblack-forest-labs/FLUX.1-schnell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_safety_checker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39murl)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/resources/images.py:76\u001b[0m, in \u001b[0;36mImages.generate\u001b[0;34m(self, prompt, model, steps, seed, n, height, width, negative_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[1;32m     61\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m ImageRequest(\n\u001b[1;32m     65\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     66\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     74\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ImageResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:249\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m     options: TogetherRequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ]:\n\u001b[1;32m    242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    243\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    244\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[1;32m    245\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    246\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:635\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     content \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    642\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:707\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(resp, rcode, stream_error\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "client = Together()\n",
    "\n",
    "response = client.images.generate(\n",
    "    prompt=\"a flying cat\",\n",
    "    model=\"black-forest-labs/FLUX.1-schnell\",\n",
    "    steps=4,\n",
    "    disable_safety_checker=True,\n",
    ")\n",
    "\n",
    "print(response.data[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtogether\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Together\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Together()\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCats eating popcorn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblack-forest-labs/FLUX.1-dev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mb64_json)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/resources/images.py:76\u001b[0m, in \u001b[0;36mImages.generate\u001b[0;34m(self, prompt, model, steps, seed, n, height, width, negative_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[1;32m     61\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m ImageRequest(\n\u001b[1;32m     65\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     66\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     74\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ImageResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:249\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m     options: TogetherRequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ]:\n\u001b[1;32m    242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    243\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    244\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[1;32m    245\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    246\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:635\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     content \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    642\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ab312/lib/python3.12/site-packages/together/abstract/api_requestor.py:707\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(resp, rcode, stream_error\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Error code: 500 - {\"message\": \"Internal server error\", \"type_\": \"server_error\"}"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together()\n",
    "response = client.images.generate(\n",
    "    prompt=\"Cats eating popcorn\",\n",
    "    model=\"black-forest-labs/FLUX.1-dev\",\n",
    "    steps=10,\n",
    "    n=4\n",
    ")\n",
    "print(response.data[0].b64_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco, the City by the Bay, is a popular tourist destination known for its iconic landmarks, vibrant neighborhoods, and stunning natural beauty. Here are the top things to do in San Francisco:\n",
      "\n",
      "1. **Golden Gate Bridge**: Take a walk or bike ride across the iconic suspension bridge, offering breathtaking views of the San Francisco Bay and the city skyline.\n",
      "2. **Alcatraz Island**: Visit the former maximum-security prison turned national park, featuring a guided tour and stunning views of the Bay.\n",
      "3. **Fisherman's Wharf**: Explore the bustling waterfront district, known for its seafood restaurants, street performers, and souvenir shops.\n",
      "4. **Chinatown**: Wander through the vibrant and colorful streets of San Francisco's Chinatown, one of the largest and oldest in the United States.\n",
      "5. **Union Square**: Shop and dine in this bustling square, surrounded by iconic department stores, art galleries, and theaters.\n",
      "6. **Lombard Street**: Take a stroll down the \"crookedest street in the world,\" offering stunning views of the city and the Bay.\n",
      "7. **Golden Gate Park**: Visit the sprawling urban park, featuring several attractions, including the de Young Museum, California Academy of Sciences, and the Japanese Tea Garden.\n",
      "8. **Cable Cars**: Ride the historic cable cars, a San Francisco icon, to explore the city's hills and neighborhoods.\n",
      "9. **Ferry Building Marketplace**: Sample artisanal foods and drinks at this bustling marketplace, featuring local farmers, cheesemongers, and bakeries.\n",
      "10. **Haight-Ashbury**: Explore the historic neighborhood, known for its 1960s counterculture movement, vintage shops, and eclectic cafes.\n",
      "11. **The Painted Ladies**: Admire the colorful Victorian houses, a iconic San Francisco landmark, on Alamo Square.\n",
      "12. **Museums**: Visit the San Francisco Museum of Modern Art (SFMOMA), the de Young Museum, or the Legion of Honor, featuring an impressive collection of art and artifacts.\n",
      "13. **Take a Food Tour**: Sample the city's diverse culinary scene, from seafood to sourdough bread, on a guided food tour.\n",
      "14. **Explore Neighborhoods**: Discover the unique character of San Francisco's neighborhoods, such as the Mission District, North Beach, and the Castro.\n",
      "15. **Day Trips**: Visit nearby attractions, like Muir Woods, Napa Valley, or Silicon Valley, for a day of wine tasting, hiking, or tech exploration.\n",
      "16. **Attend a Sporting Event**: Catch a game by the San Francisco Giants (baseball) or 49ers (football) at Oracle Park or Levi's Stadium.\n",
      "17. **Visit the Zoo**: Spend a day at the San Francisco Zoo, home to over 250 species of animals, including penguins, giraffes, and lions.\n",
      "18. **Take a Bay Cruise**: Enjoy a relaxing cruise on the San Francisco Bay, offering stunning views of the city skyline and the Golden Gate Bridge.\n",
      "19. **Explore the Presidio**: Visit the former military base turned national park, featuring hiking trails, scenic overlooks, and the Walt Disney Family Museum.\n",
      "20. **Attend a Festival or Event**: San Francisco hosts various festivals throughout the year, such as Outside Lands, SF Pride, and the San Francisco Jazz Festival.\n",
      "\n",
      "These are just a few of the many amazing things to do in San Francisco. With so much to see and do, you're sure to have an unforgettable experience in this vibrant and beautiful city.\n",
      "Paris is the capital and most populous city of France.\n"
     ]
    }
   ],
   "source": [
    "import os, asyncio\n",
    "from together import AsyncTogether\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "messages = [\n",
    "    \"What are the top things to do in San Francisco?\",\n",
    "    \"What country is Paris in?\",\n",
    "]\n",
    "\n",
    "async def async_chat_completion(messages):\n",
    "    async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "    tasks = [\n",
    "        async_client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        )\n",
    "        for message in messages\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    for response in responses:\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "asyncio.run(async_chat_completion(messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.together.ai/imgproxy/drNN5ZL8IcVzm1jzMqp31hg6jbpyVmo5AmdxUnm2I4Q/format:jpeg/aHR0cHM6Ly90b2dldGhlci1haS1iZmwtaW1hZ2VzLXByb2QuczMudXMtd2VzdC0yLmFtYXpvbmF3cy5jb20vaW1hZ2VzLzM0NmU0MGQ5NGI0M2JlZjg3YmUzYjdlZDNhYTgyZjFiMmZlMzUwYWE5MTUwZWEwYzg2MDIwNGZlNzhjZjVmOTk_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\n",
      "https://api.together.ai/imgproxy/AB7tLA8nyEqjB3aD9ULdHw-Y1teSNtNa-zsA3LKg65M/format:jpeg/aHR0cHM6Ly90b2dldGhlci1haS1iZmwtaW1hZ2VzLXByb2QuczMudXMtd2VzdC0yLmFtYXpvbmF3cy5jb20vaW1hZ2VzL2I4ZGM3M2U1NGFiNzYxZmY2ZTY0M2E0YWJlYzIwOWEzODYzMDRhZmYxMmQzZTI4YjlhY2IzZWE4YWM3MjI4NTA_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\n"
     ]
    }
   ],
   "source": [
    "# List of prompts for image generation\n",
    "prompts = [\n",
    "    \"A futuristic cityscape at dusk\",\n",
    "    \"A serene mountain landscape in autumn\",\n",
    "]\n",
    "\n",
    "# Asynchronous function to generate images for each prompt\n",
    "async def async_image_generation(prompts):\n",
    "    tasks = [\n",
    "        async_client.images.generate(\n",
    "            prompt=prompt,\n",
    "            model=\"black-forest-labs/FLUX.1-schnell\",\n",
    "            width=576,   # 9:16 aspect ratio\n",
    "            height=1024,\n",
    "            steps=4,\n",
    "            n=1,\n",
    "        )\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Print the URL of each generated image\n",
    "    for response in responses:\n",
    "        print(response.data[0].url)\n",
    "\n",
    "# Run the asynchronous image generation function\n",
    "asyncio.run(async_image_generation(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "response = client.images.generate(\n",
    "    prompt=\"space robots\",\n",
    "    model=\"black-forest-labs/FLUX.1-schnell\",\n",
    "    steps=10,\n",
    "    n=2,\n",
    ")\n",
    "print(response.data[0].b64_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ab312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
